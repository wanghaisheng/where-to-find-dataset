---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Shilong Li et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-28 11:38:04'
tags:
- all search terms
- dataset
theme: light
title: 2DDPO Scaling Direct Preference Optimization with 2Dimensional Supervision
---

# title: 2DDPO Scaling Direct Preference Optimization with 2Dimensional Supervision 
## publish date: 
**2024-10-25** 
## authors: 
  Shilong Li et.al. 
## paper id
2410.19720v1
## download
[2410.19720v1](http://arxiv.org/abs/2410.19720v1)
## abstracts:
Recent advancements in Direct Preference Optimization (DPO) have significantly enhanced the alignment of Large Language Models (LLMs) with human preferences, owing to its simplicity and effectiveness. However, existing methods typically optimize a scalar score or ranking reward, thereby overlooking the multi-dimensional nature of human preferences. In this work, we propose to extend the preference of DPO to two dimensions: segments and aspects. We first introduce a 2D supervision dataset called HelpSteer-2D. For the segment dimension, we divide the response into sentences and assign scores to each segment. For the aspect dimension, we meticulously design several criteria covering the response quality rubrics. With the 2-dimensional signals as feedback, we develop a 2D-DPO framework, decomposing the overall objective into multi-segment and multi-aspect objectives. Extensive experiments on popular benchmarks demonstrate that 2D-DPO performs better than methods that optimize for scalar or 1-dimensional preferences.
## QA:
coming soon
