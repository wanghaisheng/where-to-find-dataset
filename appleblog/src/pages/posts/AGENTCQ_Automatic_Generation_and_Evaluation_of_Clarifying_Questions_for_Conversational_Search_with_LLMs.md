---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Clemencia Siro et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-28 11:38:05'
tags:
- all search terms
- dataset
theme: light
title: AGENTCQ Automatic Generation and Evaluation of Clarifying Questions for Conversational
  Search with LLMs
---

# title: AGENTCQ Automatic Generation and Evaluation of Clarifying Questions for Conversational Search with LLMs 
## publish date: 
**2024-10-25** 
## authors: 
  Clemencia Siro et.al. 
## paper id
2410.19692v1
## download
[2410.19692v1](http://arxiv.org/abs/2410.19692v1)
## abstracts:
Generating diverse and effective clarifying questions is crucial for improving query understanding and retrieval performance in open-domain conversational search (CS) systems. We propose AGENT-CQ (Automatic GENeration, and evaluaTion of Clarifying Questions), an end-to-end LLM-based framework addressing the challenges of scalability and adaptability faced by existing methods that rely on manual curation or template-based approaches. AGENT-CQ consists of two stages: a generation stage employing LLM prompting strategies to generate clarifying questions, and an evaluation stage (CrowdLLM) that simulates human crowdsourcing judgments using multiple LLM instances to assess generated questions and answers based on comprehensive quality metrics. Extensive experiments on the ClariQ dataset demonstrate CrowdLLM's effectiveness in evaluating question and answer quality. Human evaluation and CrowdLLM show that the AGENT-CQ - generation stage, consistently outperforms baselines in various aspects of question and answer quality. In retrieval-based evaluation, LLM-generated questions significantly enhance retrieval effectiveness for both BM25 and cross-encoder models compared to human-generated questions.
## QA:
coming soon
