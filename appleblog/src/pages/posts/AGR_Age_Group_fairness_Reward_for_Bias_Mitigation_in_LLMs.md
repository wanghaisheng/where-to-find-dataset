---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Shuirong Cao et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-09-09 11:34:18'
tags:
- dataset
- all search terms
theme: light
title: AGR Age Group fairness Reward for Bias Mitigation in LLMs
---

# title: AGR Age Group fairness Reward for Bias Mitigation in LLMs 
## publish date: 
**2024-09-06** 
## authors: 
  Shuirong Cao et.al. 
## paper id
2409.04340v1
## download
[2409.04340v1](http://arxiv.org/abs/2409.04340v1)
## abstracts:
LLMs can exhibit age biases, resulting in unequal treatment of individuals across age groups. While much research has addressed racial and gender biases, age bias remains little explored. The scarcity of instruction-tuning and preference datasets for age bias hampers its detection and measurement, and existing fine-tuning methods seldom address age-related fairness. In this paper, we construct age bias preference datasets and instruction-tuning datasets for RLHF. We introduce ARG, an age fairness reward to reduce differences in the response quality of LLMs across different age groups. Extensive experiments demonstrate that this reward significantly improves response accuracy and reduces performance disparities across age groups. Our source code and datasets are available at the anonymous \href{https://anonymous.4open.science/r/FairRLHF-D445/readme.md}{link}.
## QA:
coming soon
