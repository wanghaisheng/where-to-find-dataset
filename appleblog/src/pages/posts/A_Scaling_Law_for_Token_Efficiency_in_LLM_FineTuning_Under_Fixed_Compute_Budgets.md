---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Ryan Lagasse et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-05-12 11:43:53'
tags:
- dataset
- all search terms
theme: light
title: A Scaling Law for Token Efficiency in LLM FineTuning Under Fixed Compute Budgets
---

# title: A Scaling Law for Token Efficiency in LLM FineTuning Under Fixed Compute Budgets 
## publish date: 
**2025-05-09** 
## authors: 
  Ryan Lagasse et.al. 
## paper id
2505.06150v1
## download
[2505.06150v1](http://arxiv.org/abs/2505.06150v1)
## abstracts:
We introduce a scaling law for fine-tuning large language models (LLMs) under fixed compute budgets that explicitly accounts for data composition. Conventional approaches measure training data solely by total tokens, yet the number of examples and their average token length -- what we term \emph{dataset volume} -- play a decisive role in model performance. Our formulation is tuned following established procedures. Experiments on the BRICC dataset \cite{salavati2024reducing} and subsets of the MMLU dataset \cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple subsampling strategies, reveal that data composition significantly affects token efficiency. These results motivate refined scaling laws for practical LLM fine-tuning in resource-constrained settings.
## QA:
coming soon
