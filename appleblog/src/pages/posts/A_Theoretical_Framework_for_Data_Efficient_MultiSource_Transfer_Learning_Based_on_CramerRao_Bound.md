---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Qingyue Zhang et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-02-10 11:35:44'
tags:
- all search terms
- dataset
theme: light
title: A Theoretical Framework for Data Efficient MultiSource Transfer Learning Based
  on CramerRao Bound
---

# title: A Theoretical Framework for Data Efficient MultiSource Transfer Learning Based on CramerRao Bound 
## publish date: 
**2025-02-06** 
## authors: 
  Qingyue Zhang et.al. 
## paper id
2502.04242v1
## download
[2502.04242v1](http://arxiv.org/abs/2502.04242v1)
## abstracts:
Multi-source transfer learning provides an effective solution to data scarcity in real-world supervised learning scenarios by leveraging multiple source tasks. In this field, existing works typically use all available samples from sources in training, which constrains their training efficiency and may lead to suboptimal results. To address this, we propose a theoretical framework that answers the question: what is the optimal quantity of source samples needed from each source task to jointly train the target model? Specifically, we introduce a generalization error measure that aligns with cross-entropy loss, and minimize it based on the Cram\'er-Rao Bound to determine the optimal transfer quantity for each source task. Additionally, we develop an architecture-agnostic and data-efficient algorithm OTQMS to implement our theoretical results for training deep multi-source transfer learning models. Experimental studies on diverse architectures and two real-world benchmark datasets show that our proposed algorithm significantly outperforms state-of-the-art approaches in both accuracy and data efficiency. The code and supplementary materials are available in https://anonymous.4open.science/r/Materials.
## QA:
coming soon
