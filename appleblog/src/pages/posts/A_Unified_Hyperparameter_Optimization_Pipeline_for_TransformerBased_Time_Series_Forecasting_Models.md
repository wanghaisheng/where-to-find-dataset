---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Jingjing Xu et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-01-06 11:36:58'
tags:
- all search terms
- dataset
theme: light
title: A Unified Hyperparameter Optimization Pipeline for TransformerBased Time Series
  Forecasting Models
---

# title: A Unified Hyperparameter Optimization Pipeline for TransformerBased Time Series Forecasting Models 
## publish date: 
**2025-01-02** 
## authors: 
  Jingjing Xu et.al. 
## paper id
2501.01394v1
## download
[2501.01394v1](http://arxiv.org/abs/2501.01394v1)
## abstracts:
Transformer-based models for time series forecasting (TSF) have attracted significant attention in recent years due to their effectiveness and versatility. However, these models often require extensive hyperparameter optimization (HPO) to achieve the best possible performance, and a unified pipeline for HPO in transformer-based TSF remains lacking. In this paper, we present one such pipeline and conduct extensive experiments on several state-of-the-art (SOTA) transformer-based TSF models. These experiments are conducted on standard benchmark datasets to evaluate and compare the performance of different models, generating practical insights and examples. Our pipeline is generalizable beyond transformer-based architectures and can be applied to other SOTA models, such as Mamba and TimeMixer, as demonstrated in our experiments. The goal of this work is to provide valuable guidance to both industry practitioners and academic researchers in efficiently identifying optimal hyperparameters suited to their specific domain applications. The code and complete experimental results are available on GitHub.
## QA:
coming soon
