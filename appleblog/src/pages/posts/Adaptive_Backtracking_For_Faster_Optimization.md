---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Joao V. Cavalcanti et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-08-26 11:33:38'
tags:
- dataset
- all search terms
theme: light
title: Adaptive Backtracking For Faster Optimization
---

# title: Adaptive Backtracking For Faster Optimization 
## publish date: 
**2024-08-23** 
## authors: 
  Joao V. Cavalcanti et.al. 
## paper id
2408.13150v1
## download
[2408.13150v1](http://arxiv.org/abs/2408.13150v1)
## abstracts:
Backtracking line search is foundational in numerical optimization. The basic idea is to adjust the step size of an algorithm by a constant factor until some chosen criterion (e.g. Armijo, Goldstein, Descent Lemma) is satisfied. We propose a new way for adjusting step sizes, replacing the constant factor used in regular backtracking with one that takes into account the degree to which the chosen criterion is violated, without additional computational burden. For convex problems, we prove adaptive backtracking requires fewer adjustments to produce a feasible step size than regular backtracking does for two popular line search criteria: the Armijo condition and the descent lemma. For nonconvex smooth problems, we additionally prove adaptive backtracking enjoys the same guarantees of regular backtracking. Finally, we perform a variety of experiments on over fifteen real world datasets, all of which confirm that adaptive backtracking often leads to significantly faster optimization.
## QA:
coming soon
