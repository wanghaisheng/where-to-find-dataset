---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Mukur Gupta et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-06-09 11:46:03'
tags:
- all search terms
- dataset on github
theme: light
title: AdvSumm Adversarial Training for Bias Mitigation in Text Summarization
---

# title: AdvSumm Adversarial Training for Bias Mitigation in Text Summarization 
## publish date: 
**2025-06-06** 
## authors: 
  Mukur Gupta et.al. 
## paper id
2506.06273v1
## download
[2506.06273v1](http://arxiv.org/abs/2506.06273v1)
## abstracts:
Large Language Models (LLMs) have achieved impressive performance in text summarization and are increasingly deployed in real-world applications. However, these systems often inherit associative and framing biases from pre-training data, leading to inappropriate or unfair outputs in downstream tasks. In this work, we present AdvSumm (Adversarial Summarization), a domain-agnostic training framework designed to mitigate bias in text summarization through improved generalization. Inspired by adversarial robustness, AdvSumm introduces a novel Perturber component that applies gradient-guided perturbations at the embedding level of Sequence-to-Sequence models, enhancing the model's robustness to input variations. We empirically demonstrate that AdvSumm effectively reduces different types of bias in summarization-specifically, name-nationality bias and political framing bias-without compromising summarization quality. Compared to standard transformers and data augmentation techniques like back-translation, AdvSumm achieves stronger bias mitigation performance across benchmark datasets.
## QA:
coming soon
