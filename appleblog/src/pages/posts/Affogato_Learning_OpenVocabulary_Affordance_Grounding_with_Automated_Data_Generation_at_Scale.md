---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Junha Lee et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-06-16 11:46:05'
tags:
- all search terms
- dataset
theme: light
title: Affogato Learning OpenVocabulary Affordance Grounding with Automated Data Generation
  at Scale
---

# title: Affogato Learning OpenVocabulary Affordance Grounding with Automated Data Generation at Scale 
## publish date: 
**2025-06-13** 
## authors: 
  Junha Lee et.al. 
## paper id
2506.12009v1
## download
[2506.12009v1](http://arxiv.org/abs/2506.12009v1)
## abstracts:
Affordance grounding-localizing object regions based on natural language descriptions of interactions-is a critical challenge for enabling intelligent agents to understand and interact with their environments. However, this task remains challenging due to the need for fine-grained part-level localization, the ambiguity arising from multiple valid interaction regions, and the scarcity of large-scale datasets. In this work, we introduce Affogato, a large-scale benchmark comprising 150K instances, annotated with open-vocabulary text descriptions and corresponding 3D affordance heatmaps across a diverse set of objects and interactions. Building on this benchmark, we develop simple yet effective vision-language models that leverage pretrained part-aware vision backbones and a text-conditional heatmap decoder. Our models trained with the Affogato dataset achieve promising performance on the existing 2D and 3D benchmarks, and notably, exhibit effectiveness in open-vocabulary cross-domain generalization. The Affogato dataset is shared in public: https://huggingface.co/datasets/project-affogato/affogato
## QA:
coming soon
