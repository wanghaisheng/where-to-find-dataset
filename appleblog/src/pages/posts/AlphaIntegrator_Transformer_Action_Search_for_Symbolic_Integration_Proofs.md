---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: "Mert \xDCnsal et.al."
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-07 11:36:41'
tags:
- dataset
- all search terms
theme: light
title: AlphaIntegrator Transformer Action Search for Symbolic Integration Proofs
---

# title: AlphaIntegrator Transformer Action Search for Symbolic Integration Proofs 
## publish date: 
**2024-10-03** 
## authors: 
  Mert Ãœnsal et.al. 
## paper id
2410.02666v1
## download
[2410.02666v1](http://arxiv.org/abs/2410.02666v1)
## abstracts:
We present the first correct-by-construction learning-based system for step-by-step mathematical integration. The key idea is to learn a policy, represented by a GPT transformer model, which guides the search for the right mathematical integration rule, to be carried out by a symbolic solver. Concretely, we introduce a symbolic engine with axiomatically correct actions on mathematical expressions, as well as the first dataset for step-by-step integration. Our GPT-style transformer model, trained on this synthetic data, demonstrates strong generalization by surpassing its own data generator in accuracy and efficiency, using 50% fewer search steps. Our experimental results with SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs on a set of question-answer pairs is insufficient for solving this mathematical task. This motivates the importance of discovering creative methods for combining LLMs with symbolic reasoning engines, of which our work is an instance.
## QA:
coming soon
