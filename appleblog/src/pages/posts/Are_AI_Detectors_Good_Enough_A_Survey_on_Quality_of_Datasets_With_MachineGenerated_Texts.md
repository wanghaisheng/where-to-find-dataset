---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: German Gritsai et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-21 11:37:21'
tags:
- all search terms
- dataset
theme: light
title: Are AI Detectors Good Enough A Survey on Quality of Datasets With MachineGenerated
  Texts
---

# title: Are AI Detectors Good Enough A Survey on Quality of Datasets With MachineGenerated Texts 
## publish date: 
**2024-10-18** 
## authors: 
  German Gritsai et.al. 
## paper id
2410.14677v1
## download
[2410.14677v1](http://arxiv.org/abs/2410.14677v1)
## abstracts:
The rapid development of autoregressive Large Language Models (LLMs) has significantly improved the quality of generated texts, necessitating reliable machine-generated text detectors. A huge number of detectors and collections with AI fragments have emerged, and several detection methods even showed recognition quality up to 99.9% according to the target metrics in such collections. However, the quality of such detectors tends to drop dramatically in the wild, posing a question: Are detectors actually highly trustworthy or do their high benchmark scores come from the poor quality of evaluation datasets? In this paper, we emphasise the need for robust and qualitative methods for evaluating generated data to be secure against bias and low generalising ability of future model. We present a systematic review of datasets from competitions dedicated to AI-generated content detection and propose methods for evaluating the quality of datasets containing AI-generated fragments. In addition, we discuss the possibility of using high-quality generated data to achieve two goals: improving the training of detection models and improving the training datasets themselves. Our contribution aims to facilitate a better understanding of the dynamics between human and machine text, which will ultimately support the integrity of information in an increasingly automated world.
## QA:
coming soon
