---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Iuliia Zaitova et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-05-12 11:43:56'
tags:
- dataset
- all search terms
theme: light
title: Attention on Multiword Expressions A Multilingual Study of BERTbased Models
  with Regard to Idiomaticity and Microsyntax
---

# title: Attention on Multiword Expressions A Multilingual Study of BERTbased Models with Regard to Idiomaticity and Microsyntax 
## publish date: 
**2025-05-09** 
## authors: 
  Iuliia Zaitova et.al. 
## paper id
2505.06062v1
## download
[2505.06062v1](http://arxiv.org/abs/2505.06062v1)
## abstracts:
This study analyzes the attention patterns of fine-tuned encoder-only models based on the BERT architecture (BERT-based models) towards two distinct types of Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms present challenges in semantic non-compositionality, whereas MSUs demonstrate unconventional syntactic behavior that does not conform to standard grammatical categorizations. We aim to understand whether fine-tuning BERT-based models on specific tasks influences their attention to MWEs, and how this attention differs between semantic and syntactic tasks. We examine attention scores to MWEs in both pre-trained and fine-tuned BERT-based models. We utilize monolingual models and datasets in six Indo-European languages - English, German, Dutch, Polish, Russian, and Ukrainian. Our results show that fine-tuning significantly influences how models allocate attention to MWEs. Specifically, models fine-tuned on semantic tasks tend to distribute attention to idiomatic expressions more evenly across layers. Models fine-tuned on syntactic tasks show an increase in attention to MSUs in the lower layers, corresponding with syntactic processing requirements.
## QA:
coming soon
