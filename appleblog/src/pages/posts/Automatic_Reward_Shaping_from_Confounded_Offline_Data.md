---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Mingxuan Li et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-05-19 11:44:49'
tags:
- dataset on github
- all search terms
theme: light
title: Automatic Reward Shaping from Confounded Offline Data
---

# title: Automatic Reward Shaping from Confounded Offline Data 
## publish date: 
**2025-05-16** 
## authors: 
  Mingxuan Li et.al. 
## paper id
2505.11478v1
## download
[2505.11478v1](http://arxiv.org/abs/2505.11478v1)
## abstracts:
A key task in Artificial Intelligence is learning effective policies for controlling agents in unknown environments to optimize performance measures. Off-policy learning methods, like Q-learning, allow learners to make optimal decisions based on past experiences. This paper studies off-policy learning from biased data in complex and high-dimensional domains where \emph{unobserved confounding} cannot be ruled out a priori. Building on the well-celebrated Deep Q-Network (DQN), we propose a novel deep reinforcement learning algorithm robust to confounding biases in observed data. Specifically, our algorithm attempts to find a safe policy for the worst-case environment compatible with the observations. We apply our method to twelve confounded Atari games, and find that it consistently dominates the standard DQN in all games where the observed input to the behavioral and target policies mismatch and unobserved confounders exist.
## QA:
coming soon
