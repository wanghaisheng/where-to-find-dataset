---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Rachel Longjohn et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-11-04 11:37:42'
tags:
- all search terms
- dataset
theme: light
title: Benchmark Data Repositories for Better Benchmarking
---

# title: Benchmark Data Repositories for Better Benchmarking 
## publish date: 
**2024-10-31** 
## authors: 
  Rachel Longjohn et.al. 
## paper id
2410.24100v1
## download
[2410.24100v1](http://arxiv.org/abs/2410.24100v1)
## abstracts:
In machine learning research, it is common to evaluate algorithms via their performance on standard benchmark datasets. While a growing body of work establishes guidelines for -- and levies criticisms at -- data and benchmarking practices in machine learning, comparatively less attention has been paid to the data repositories where these datasets are stored, documented, and shared. In this paper, we analyze the landscape of these $\textit{benchmark data repositories}$ and the role they can play in improving benchmarking. This role includes addressing issues with both datasets themselves (e.g., representational harms, construct validity) and the manner in which evaluation is carried out using such datasets (e.g., overemphasis on a few datasets and metrics, lack of reproducibility). To this end, we identify and discuss a set of considerations surrounding the design and use of benchmark data repositories, with a focus on improving benchmarking practices in machine learning.
## QA:
coming soon
