---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Sarubi Thillainathan et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-03-31 11:40:21'
tags:
- all search terms
- dataset
theme: light
title: Beyond Vanilla FineTuning Leveraging Multistage Multilingual and DomainSpecific
  Methods for LowResource Machine Translation
---

# title: Beyond Vanilla FineTuning Leveraging Multistage Multilingual and DomainSpecific Methods for LowResource Machine Translation 
## publish date: 
**2025-03-28** 
## authors: 
  Sarubi Thillainathan et.al. 
## paper id
2503.22582v1
## download
[2503.22582v1](http://arxiv.org/abs/2503.22582v1)
## abstracts:
Fine-tuning multilingual sequence-to-sequence large language models (msLLMs) has shown promise in developing neural machine translation (NMT) systems for low-resource languages (LRLs). However, conventional single-stage fine-tuning methods struggle in extremely low-resource NMT settings, where training data is very limited. This paper contributes to artificial intelligence by proposing two approaches for adapting msLLMs in these challenging scenarios: (1) continual pre-training (CPT), where the msLLM is further trained with domain-specific monolingual data to compensate for the under-representation of LRLs, and (2) intermediate task transfer learning (ITTL), a method that fine-tunes the msLLM with both in-domain and out-of-domain parallel data to enhance its translation capabilities across various domains and tasks. As an application in engineering, these methods are implemented in NMT systems for Sinhala, Tamil, and English (six language pairs) in domain-specific, extremely low-resource settings (datasets containing fewer than 100,000 samples). Our experiments reveal that these approaches enhance translation performance by an average of +1.47 bilingual evaluation understudy (BLEU) score compared to the standard single-stage fine-tuning baseline across all translation directions. Additionally, a multi-model ensemble further improves performance by an additional BLEU score.
## QA:
coming soon
