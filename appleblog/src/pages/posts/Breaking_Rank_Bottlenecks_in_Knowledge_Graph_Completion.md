---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Samy Badreddine et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-06-30 11:47:23'
tags:
- all search terms
- dataset
theme: light
title: Breaking Rank Bottlenecks in Knowledge Graph Completion
---

# title: Breaking Rank Bottlenecks in Knowledge Graph Completion 
## publish date: 
**2025-06-27** 
## authors: 
  Samy Badreddine et.al. 
## paper id
2506.22271v1
## download
[2506.22271v1](http://arxiv.org/abs/2506.22271v1)
## abstracts:
Many Knowledge Graph Completion (KGC) models, despite using powerful encoders, rely on a simple vector-matrix multiplication to score queries against candidate object entities. When the number of entities is larger than the model's embedding dimension, which in practical scenarios is often by several orders of magnitude, we have a linear output layer with a rank bottleneck. Such bottlenecked layers limit model expressivity. We investigate both theoretically and empirically how rank bottlenecks affect KGC models. We find that, by limiting the set of feasible predictions, rank bottlenecks hurt ranking accuracy and the distribution fidelity of scores. Inspired by the language modelling literature, we propose KGE-MoS, a mixture-based output layer to break rank bottlenecks in many KGC models. Our experiments on four datasets show that KGE-MoS improves performance and probabilistic fit of KGC models for a low parameter cost.
## QA:
coming soon
