---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Gaifan Zhang et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-03-24 11:39:29'
tags:
- dataset
- all search terms
theme: light
title: CASE ConditionAware Sentence Embeddings for Conditional Semantic Textual Similarity
  Measurement
---

# title: CASE ConditionAware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement 
## publish date: 
**2025-03-21** 
## authors: 
  Gaifan Zhang et.al. 
## paper id
2503.17279v1
## download
[2503.17279v1](http://arxiv.org/abs/2503.17279v1)
## abstracts:
The meaning conveyed by a sentence often depends on the context in which it appears. Despite the progress of sentence embedding methods, it remains unclear how to best modify a sentence embedding conditioned on its context. To address this problem, we propose Condition-Aware Sentence Embeddings (CASE), an efficient and accurate method to create an embedding for a sentence under a given condition. First, CASE creates an embedding for the condition using a Large Language Model (LLM), where the sentence influences the attention scores computed for the tokens in the condition during pooling. Next, a supervised nonlinear projection is learned to reduce the dimensionality of the LLM-based text embeddings. We show that CASE significantly outperforms previously proposed Conditional Semantic Textual Similarity (C-STS) methods on an existing standard benchmark dataset. We find that subtracting the condition embedding consistently improves the C-STS performance of LLM-based text embeddings. Moreover, we propose a supervised dimensionality reduction method that not only reduces the dimensionality of LLM-based embeddings but also significantly improves their performance.
## QA:
coming soon
