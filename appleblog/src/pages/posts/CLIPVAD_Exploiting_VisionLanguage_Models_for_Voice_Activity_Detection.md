---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Andrea Appiani et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-21 11:37:26'
tags:
- all search terms
- dataset
theme: light
title: CLIPVAD Exploiting VisionLanguage Models for Voice Activity Detection
---

# title: CLIPVAD Exploiting VisionLanguage Models for Voice Activity Detection 
## publish date: 
**2024-10-18** 
## authors: 
  Andrea Appiani et.al. 
## paper id
2410.14509v1
## download
[2410.14509v1](http://arxiv.org/abs/2410.14509v1)
## abstracts:
Voice Activity Detection (VAD) is the process of automatically determining whether a person is speaking and identifying the timing of their speech in an audiovisual data. Traditionally, this task has been tackled by processing either audio signals or visual data, or by combining both modalities through fusion or joint learning. In our study, drawing inspiration from recent advancements in visual-language models, we introduce a novel approach leveraging Contrastive Language-Image Pretraining (CLIP) models. The CLIP visual encoder analyzes video segments composed of the upper body of an individual, while the text encoder handles textual descriptions automatically generated through prompt engineering. Subsequently, embeddings from these encoders are fused through a deep neural network to perform VAD. Our experimental analysis across three VAD benchmarks showcases the superior performance of our method compared to existing visual VAD approaches. Notably, our approach outperforms several audio-visual methods despite its simplicity, and without requiring pre-training on extensive audio-visual datasets.
## QA:
coming soon
