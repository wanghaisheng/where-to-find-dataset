---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Filippo Merlo et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-06-30 11:47:23'
tags:
- all search terms
- dataset
theme: light
title: COOCO Common Objects OutofContext Semantic Violation in Scenes Investigating
  Multimodal Context in Referential Communication
---

# title: COOCO Common Objects OutofContext Semantic Violation in Scenes Investigating Multimodal Context in Referential Communication 
## publish date: 
**2025-06-27** 
## authors: 
  Filippo Merlo et.al. 
## paper id
2506.22274v1
## download
[2506.22274v1](http://arxiv.org/abs/2506.22274v1)
## abstracts:
Natural scenes provide us with rich contexts for object recognition and reference. In particular, knowing what type of scene one is looking at generates expectations about which objects will occur, and what their spatial configuration should be. Do Vision-Language Models (VLMs) learn to rely on scene contexts in a similar way, when generating references to objects? To address this question, we introduce the \textit{Common Objects Out-of-Context (COOCO)} dataset and test to what extent VLMs rely on scene context to refer to objects under different degrees of scene-object congruency, and different perturbations. Our findings show that models leverage scene context adaptively, depending on both the semantic relatedness between object and scene and the level of noise. In particular, models rely more on context under high target-scene congruence or when objects are degraded. Attention analysis reveals that successful object categorisation involves increased focus on the target in mid-level layers, especially under moderate noise, suggesting that VLMs dynamically balance local and contextual information for reference generation. We make our dataset, code and models available at \href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.
## QA:
coming soon
