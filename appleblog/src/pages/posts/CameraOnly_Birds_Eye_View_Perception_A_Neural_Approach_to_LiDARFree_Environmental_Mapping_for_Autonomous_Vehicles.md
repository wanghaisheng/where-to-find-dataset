---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Anupkumar Bochare et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-05-12 11:43:55'
tags:
- dataset
- all search terms
theme: light
title: CameraOnly Birds Eye View Perception A Neural Approach to LiDARFree Environmental
  Mapping for Autonomous Vehicles
---

# title: CameraOnly Birds Eye View Perception A Neural Approach to LiDARFree Environmental Mapping for Autonomous Vehicles 
## publish date: 
**2025-05-09** 
## authors: 
  Anupkumar Bochare et.al. 
## paper id
2505.06113v1
## download
[2505.06113v1](http://arxiv.org/abs/2505.06113v1)
## abstracts:
Autonomous vehicle perception systems have traditionally relied on costly LiDAR sensors to generate precise environmental representations. In this paper, we propose a camera-only perception framework that produces Bird's Eye View (BEV) maps by extending the Lift-Splat-Shoot architecture. Our method combines YOLOv11-based object detection with DepthAnythingV2 monocular depth estimation across multi-camera inputs to achieve comprehensive 360-degree scene understanding. We evaluate our approach on the OpenLane-V2 and NuScenes datasets, achieving up to 85% road segmentation accuracy and 85-90% vehicle detection rates when compared against LiDAR ground truth, with average positional errors limited to 1.2 meters. These results highlight the potential of deep learning to extract rich spatial information using only camera inputs, enabling cost-efficient autonomous navigation without sacrificing accuracy.
## QA:
coming soon
