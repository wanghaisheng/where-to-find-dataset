---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Changming Xu et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-06-16 11:46:07'
tags:
- all search terms
- dataset
theme: light
title: Compression Aware Certified Training
---

# title: Compression Aware Certified Training 
## publish date: 
**2025-06-13** 
## authors: 
  Changming Xu et.al. 
## paper id
2506.11992v1
## download
[2506.11992v1](http://arxiv.org/abs/2506.11992v1)
## abstracts:
Deep neural networks deployed in safety-critical, resource-constrained environments must balance efficiency and robustness. Existing methods treat compression and certified robustness as separate goals, compromising either efficiency or safety. We propose CACTUS (Compression Aware Certified Training Using network Sets), a general framework for unifying these objectives during training. CACTUS models maintain high certified accuracy even when compressed. We apply CACTUS for both pruning and quantization and show that it effectively trains models which can be efficiently compressed while maintaining high accuracy and certifiable robustness. CACTUS achieves state-of-the-art accuracy and certified performance for both pruning and quantization on a variety of datasets and input specifications.
## QA:
coming soon
