---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Xiaodan Chen et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-06-16 11:46:10'
tags:
- all search terms
- dataset
theme: light
title: ConfidenceBased SelfTraining for EMGtoSpeech Leveraging Synthetic EMG for Robust
  Modeling
---

# title: ConfidenceBased SelfTraining for EMGtoSpeech Leveraging Synthetic EMG for Robust Modeling 
## publish date: 
**2025-06-13** 
## authors: 
  Xiaodan Chen et.al. 
## paper id
2506.11862v1
## download
[2506.11862v1](http://arxiv.org/abs/2506.11862v1)
## abstracts:
Voiced Electromyography (EMG)-to-Speech (V-ETS) models reconstruct speech from muscle activity signals, facilitating applications such as neurolaryngologic diagnostics. Despite its potential, the advancement of V-ETS is hindered by a scarcity of paired EMG-speech data. To address this, we propose a novel Confidence-based Multi-Speaker Self-training (CoM2S) approach, along with a newly curated Libri-EMG dataset. This approach leverages synthetic EMG data generated by a pre-trained model, followed by a proposed filtering mechanism based on phoneme-level confidence to enhance the ETS model through the proposed self-training techniques. Experiments demonstrate our method improves phoneme accuracy, reduces phonological confusion, and lowers word error rate, confirming the effectiveness of our CoM2S approach for V-ETS. In support of future research, we will release the codes and the proposed Libri-EMG dataset-an open-access, time-aligned, multi-speaker voiced EMG and speech recordings.
## QA:
coming soon
