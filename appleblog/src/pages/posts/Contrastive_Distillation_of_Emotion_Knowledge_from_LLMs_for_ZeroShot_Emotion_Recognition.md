---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Minxue Niu et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-05-26 11:44:19'
tags:
- dataset
- all search terms
theme: light
title: Contrastive Distillation of Emotion Knowledge from LLMs for ZeroShot Emotion
  Recognition
---

# title: Contrastive Distillation of Emotion Knowledge from LLMs for ZeroShot Emotion Recognition 
## publish date: 
**2025-05-23** 
## authors: 
  Minxue Niu et.al. 
## paper id
2505.18040v1
## download
[2505.18040v1](http://arxiv.org/abs/2505.18040v1)
## abstracts:
The ability to handle various emotion labels without dedicated training is crucial for building adaptable Emotion Recognition (ER) systems. Conventional ER models rely on training using fixed label sets and struggle to generalize beyond them. On the other hand, Large Language Models (LLMs) have shown strong zero-shot ER performance across diverse label spaces, but their scale limits their use on edge devices. In this work, we propose a contrastive distillation framework that transfers rich emotional knowledge from LLMs into a compact model without the use of human annotations. We use GPT-4 to generate descriptive emotion annotations, offering rich supervision beyond fixed label sets. By aligning text samples with emotion descriptors in a shared embedding space, our method enables zero-shot prediction on different emotion classes, granularity, and label schema. The distilled model is effective across multiple datasets and label spaces, outperforming strong baselines of similar size and approaching GPT-4's zero-shot performance, while being over 10,000 times smaller.
## QA:
coming soon
