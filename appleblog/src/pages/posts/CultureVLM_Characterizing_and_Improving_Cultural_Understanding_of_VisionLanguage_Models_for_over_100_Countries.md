---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Shudong Liu et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-01-06 11:37:04'
tags:
- all search terms
- dataset
theme: light
title: CultureVLM Characterizing and Improving Cultural Understanding of VisionLanguage
  Models for over 100 Countries
---

# title: CultureVLM Characterizing and Improving Cultural Understanding of VisionLanguage Models for over 100 Countries 
## publish date: 
**2025-01-02** 
## authors: 
  Shudong Liu et.al. 
## paper id
2501.01282v1
## download
[2501.01282v1](http://arxiv.org/abs/2501.01282v1)
## abstracts:
Vision-language models (VLMs) have advanced human-AI interaction but struggle with cultural understanding, often misinterpreting symbols, gestures, and artifacts due to biases in predominantly Western-centric training data. In this paper, we construct CultureVerse, a large-scale multimodal benchmark covering 19, 682 cultural concepts, 188 countries/regions, 15 cultural concepts, and 3 question types, with the aim of characterizing and improving VLMs' multicultural understanding capabilities. Then, we propose CultureVLM, a series of VLMs fine-tuned on our dataset to achieve significant performance improvement in cultural understanding. Our evaluation of 16 models reveals significant disparities, with a stronger performance in Western concepts and weaker results in African and Asian contexts. Fine-tuning on our CultureVerse enhances cultural perception, demonstrating cross-cultural, cross-continent, and cross-dataset generalization without sacrificing performance on models' general VLM benchmarks. We further present insights on cultural generalization and forgetting. We hope that this work could lay the foundation for more equitable and culturally aware multimodal AI systems.
## QA:
coming soon
