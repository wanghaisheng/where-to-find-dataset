---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Gao Yu Lee et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-21 11:37:24'
tags:
- all search terms
- dataset
theme: light
title: DRACODehazeNet An Efficient Image Dehazing Network Combining Detail Recovery
  and a Novel Contrastive Learning Paradigm
---

# title: DRACODehazeNet An Efficient Image Dehazing Network Combining Detail Recovery and a Novel Contrastive Learning Paradigm 
## publish date: 
**2024-10-18** 
## authors: 
  Gao Yu Lee et.al. 
## paper id
2410.14595v1
## download
[2410.14595v1](http://arxiv.org/abs/2410.14595v1)
## abstracts:
Image dehazing is crucial for clarifying images obscured by haze or fog, but current learning-based approaches is dependent on large volumes of training data and hence consumed significant computational power. Additionally, their performance is often inadequate under non-uniform or heavy haze. To address these challenges, we developed the Detail Recovery And Contrastive DehazeNet, which facilitates efficient and effective dehazing via a dense dilated inverted residual block and an attention-based detail recovery network that tailors enhancements to specific dehazed scene contexts. A major innovation is its ability to train effectively with limited data, achieved through a novel quadruplet loss-based contrastive dehazing paradigm. This approach distinctly separates hazy and clear image features while also distinguish lower-quality and higher-quality dehazed images obtained from each sub-modules of our network, thereby refining the dehazing process to a larger extent. Extensive tests on a variety of benchmarked haze datasets demonstrated the superiority of our approach. The code repository for this work will be available soon.
## QA:
coming soon
