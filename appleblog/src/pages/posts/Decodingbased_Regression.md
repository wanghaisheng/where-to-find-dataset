---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Xingyou Song et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-02-03 11:34:55'
tags:
- all search terms
- dataset on github
theme: light
title: Decodingbased Regression
---

# title: Decodingbased Regression 
## publish date: 
**2025-01-31** 
## authors: 
  Xingyou Song et.al. 
## paper id
2501.19383v1
## download
[2501.19383v1](http://arxiv.org/abs/2501.19383v1)
## abstracts:
Language models have recently been shown capable of performing regression tasks wherein numeric predictions are represented as decoded strings. In this work, we provide theoretical grounds for this capability and furthermore investigate the utility of causal auto-regressive sequence models when they are applied to any feature representation. We find that, despite being trained in the usual way - for next-token prediction via cross-entropy loss - decoding-based regression is as performant as traditional approaches for tabular regression tasks, while being flexible enough to capture arbitrary distributions, such as in the task of density estimation.
## QA:
coming soon
