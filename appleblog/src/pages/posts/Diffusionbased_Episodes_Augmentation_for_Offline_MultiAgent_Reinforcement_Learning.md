---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Jihwan Oh et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-08-26 11:33:40'
tags:
- dataset
- all search terms
theme: light
title: Diffusionbased Episodes Augmentation for Offline MultiAgent Reinforcement Learning
---

# title: Diffusionbased Episodes Augmentation for Offline MultiAgent Reinforcement Learning 
## publish date: 
**2024-08-23** 
## authors: 
  Jihwan Oh et.al. 
## paper id
2408.13092v1
## download
[2408.13092v1](http://arxiv.org/abs/2408.13092v1)
## abstracts:
Offline multi-agent reinforcement learning (MARL) is increasingly recognized as crucial for effectively deploying RL algorithms in environments where real-time interaction is impractical, risky, or costly. In the offline setting, learning from a static dataset of past interactions allows for the development of robust and safe policies without the need for live data collection, which can be fraught with challenges. Building on this foundational importance, we present EAQ, Episodes Augmentation guided by Q-total loss, a novel approach for offline MARL framework utilizing diffusion models. EAQ integrates the Q-total function directly into the diffusion model as a guidance to maximize the global returns in an episode, eliminating the need for separate training. Our focus primarily lies on cooperative scenarios, where agents are required to act collectively towards achieving a shared goal-essentially, maximizing global returns. Consequently, we demonstrate that our episodes augmentation in a collaborative manner significantly boosts offline MARL algorithm compared to the original dataset, improving the normalized return by +17.3% and +12.9% for medium and poor behavioral policies in SMAC simulator, respectively.
## QA:
coming soon
