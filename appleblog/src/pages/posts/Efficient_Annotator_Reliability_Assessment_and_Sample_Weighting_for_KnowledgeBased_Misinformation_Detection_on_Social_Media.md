---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Owen Cook et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-21 11:37:26'
tags:
- all search terms
- dataset
theme: light
title: Efficient Annotator Reliability Assessment and Sample Weighting for KnowledgeBased
  Misinformation Detection on Social Media
---

# title: Efficient Annotator Reliability Assessment and Sample Weighting for KnowledgeBased Misinformation Detection on Social Media 
## publish date: 
**2024-10-18** 
## authors: 
  Owen Cook et.al. 
## paper id
2410.14515v1
## download
[2410.14515v1](http://arxiv.org/abs/2410.14515v1)
## abstracts:
Misinformation spreads rapidly on social media, confusing the truth and targetting potentially vulnerable people. To effectively mitigate the negative impact of misinformation, it must first be accurately detected before applying a mitigation strategy, such as X's community notes, which is currently a manual process. This study takes a knowledge-based approach to misinformation detection, modelling the problem similarly to one of natural language inference. The EffiARA annotation framework is introduced, aiming to utilise inter- and intra-annotator agreement to understand the reliability of each annotator and influence the training of large language models for classification based on annotator reliability. In assessing the EffiARA annotation framework, the Russo-Ukrainian Conflict Knowledge-Based Misinformation Classification Dataset (RUC-MCD) was developed and made publicly available. This study finds that sample weighting using annotator reliability performs the best, utilising both inter- and intra-annotator agreement and soft-label training. The highest classification performance achieved using Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.
## QA:
coming soon
