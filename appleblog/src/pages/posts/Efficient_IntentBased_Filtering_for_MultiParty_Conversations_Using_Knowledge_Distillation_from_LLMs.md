---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Reem Gody et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-03-24 11:39:27'
tags:
- dataset
- all search terms
theme: light
title: Efficient IntentBased Filtering for MultiParty Conversations Using Knowledge
  Distillation from LLMs
---

# title: Efficient IntentBased Filtering for MultiParty Conversations Using Knowledge Distillation from LLMs 
## publish date: 
**2025-03-21** 
## authors: 
  Reem Gody et.al. 
## paper id
2503.17336v1
## download
[2503.17336v1](http://arxiv.org/abs/2503.17336v1)
## abstracts:
Large language models (LLMs) have showcased remarkable capabilities in conversational AI, enabling open-domain responses in chat-bots, as well as advanced processing of conversations like summarization, intent classification, and insights generation. However, these models are resource-intensive, demanding substantial memory and computational power. To address this, we propose a cost-effective solution that filters conversational snippets of interest for LLM processing, tailored to the target downstream application, rather than processing every snippet. In this work, we introduce an innovative approach that leverages knowledge distillation from LLMs to develop an intent-based filter for multi-party conversations, optimized for compute power constrained environments. Our method combines different strategies to create a diverse multi-party conversational dataset, that is annotated with the target intents and is then used to fine-tune the MobileBERT model for multi-label intent classification. This model achieves a balance between efficiency and performance, effectively filtering conversation snippets based on their intents. By passing only the relevant snippets to the LLM for further processing, our approach significantly reduces overall operational costs depending on the intents and the data distribution as demonstrated in our experiments.
## QA:
coming soon
