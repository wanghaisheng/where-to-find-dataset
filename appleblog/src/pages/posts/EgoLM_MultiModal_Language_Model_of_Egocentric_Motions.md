---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Fangzhou Hong et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-09-30 11:37:54'
tags:
- all search terms
- dataset
theme: light
title: EgoLM MultiModal Language Model of Egocentric Motions
---

# title: EgoLM MultiModal Language Model of Egocentric Motions 
## publish date: 
**2024-09-26** 
## authors: 
  Fangzhou Hong et.al. 
## paper id
2409.18127v1
## download
[2409.18127v1](http://arxiv.org/abs/2409.18127v1)
## abstracts:
As the prevalence of wearable devices, learning egocentric motions becomes essential to develop contextual AI. In this work, we present EgoLM, a versatile framework that tracks and understands egocentric motions from multi-modal inputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich contexts for the disambiguation of egomotion tracking and understanding, which are ill-posed under single modality conditions. To facilitate the versatile and multi-modal framework, our key insight is to model the joint distribution of egocentric motions and natural languages using large language models (LLM). Multi-modal sensor inputs are encoded and projected to the joint latent space of language models, and used to prompt motion generation or text generation for egomotion tracking or understanding, respectively. Extensive experiments on large-scale multi-modal human motion dataset validate the effectiveness of EgoLM as a generalist model for universal egocentric learning.
## QA:
coming soon
