---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Luke Merrick et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-07-29 11:32:45'
tags:
- dataset on github
- all search terms
theme: light
title: Embedding And Clustering Your Data Can Improve Contrastive Pretraining
---

# title: Embedding And Clustering Your Data Can Improve Contrastive Pretraining 
## publish date: 
**2024-07-26** 
## authors: 
  Luke Merrick et.al. 
## paper id
2407.18887v1
## download
[2407.18887v1](http://arxiv.org/abs/2407.18887v1)
## abstracts:
Recent studies of large-scale contrastive pretraining in the text embedding domain show that using single-source minibatches, rather than mixed-source minibatches, can substantially improve overall model accuracy. In this work, we explore extending training data stratification beyond source granularity by leveraging a pretrained text embedding model and the classic k-means clustering algorithm to further split training data apart by the semantic clusters within each source. Experimentally, we observe a notable increase in NDCG@10 when pretraining a BERT-based text embedding model on query-passage pairs from the MSMARCO passage retrieval dataset. Additionally, we conceptually connect our clustering approach to both the Topic Aware Sampling (TAS) aspect of the TAS-B methodology and the nearest-neighbor-based hard-negative mining aspect of the ANCE methodology and discuss how this unified view motivates future lines of research on the organization of contrastive pretraining data.
## QA:
coming soon
