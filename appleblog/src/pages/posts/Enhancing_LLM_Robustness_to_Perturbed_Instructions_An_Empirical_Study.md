---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Aryan Agrawal et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-04-07 11:39:46'
tags:
- all search terms
- dataset
theme: light
title: Enhancing LLM Robustness to Perturbed Instructions An Empirical Study
---

# title: Enhancing LLM Robustness to Perturbed Instructions An Empirical Study 
## publish date: 
**2025-04-03** 
## authors: 
  Aryan Agrawal et.al. 
## paper id
2504.02733v1
## download
[2504.02733v1](http://arxiv.org/abs/2504.02733v1)
## abstracts:
Large Language Models (LLMs) are highly vulnerable to input perturbations, as even a small prompt change may result in a substantially different output. Existing methods to enhance LLM robustness are primarily focused on perturbed data samples, whereas improving resiliency to perturbations of task-level instructions has remained relatively underexplored. In this work, we focus on character- and word-level edits of task-specific instructions, which substantially degrade downstream performance. We experiment with a variety of techniques to enhance the robustness of LLMs, including self-denoising and representation alignment, testing different models (Llama 3 and Flan-T5), datasets (CoLA, QNLI, SST-2) and instructions (both task-oriented and role-oriented). We find that, on average, self-denoising -- whether performed by a frozen LLM or a fine-tuned model -- achieves substantially higher performance gains than alternative strategies, including more complex baselines such as ensembling and supervised methods.
## QA:
coming soon
