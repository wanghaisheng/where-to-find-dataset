---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Han Yi et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-06-09 11:46:02'
tags:
- all search terms
- dataset on github
theme: light
title: ExAct A VideoLanguage Benchmark for Expert Action Analysis
---

# title: ExAct A VideoLanguage Benchmark for Expert Action Analysis 
## publish date: 
**2025-06-06** 
## authors: 
  Han Yi et.al. 
## paper id
2506.06277v1
## download
[2506.06277v1](http://arxiv.org/abs/2506.06277v1)
## abstracts:
We present ExAct, a new video-language benchmark for expert-level understanding of skilled physical human activities. Our new benchmark contains 3521 expert-curated video question-answer pairs spanning 11 physical activities in 6 domains: Sports, Bike Repair, Cooking, Health, Music, and Dance. ExAct requires the correct answer to be selected from five carefully designed candidate options, thus necessitating a nuanced, fine-grained, expert-level understanding of physical human skills. Evaluating the recent state-of-the-art VLMs on ExAct reveals a substantial performance gap relative to human expert performance. Specifically, the best-performing GPT-4o model achieves only 44.70% accuracy, well below the 82.02% attained by trained human specialists/experts. We believe that ExAct will be beneficial for developing and evaluating VLMs capable of precise understanding of human skills in various physical and procedural domains. Dataset and code are available at https://texaser.github.io/exact_project_page/
## QA:
coming soon
