---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: He Zhu et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-08-05 11:32:47'
tags:
- all search terms
- dataset
theme: light
title: FANNO Augmenting HighQuality Instruction Data with OpenSourced LLMs Only
---

# title: FANNO Augmenting HighQuality Instruction Data with OpenSourced LLMs Only 
## publish date: 
**2024-08-02** 
## authors: 
  He Zhu et.al. 
## paper id
2408.01323v1
## download
[2408.01323v1](http://arxiv.org/abs/2408.01323v1)
## abstracts:
Instruction fine-tuning stands as a crucial advancement in leveraging large language models (LLMs) for enhanced task performance. However, the annotation of instruction datasets has traditionally been expensive and laborious, often relying on manual annotations or costly API calls of proprietary LLMs. To address these challenges, we introduce FANNO, a fully autonomous, open-sourced framework that revolutionizes the annotation process without the need for pre-existing annotated data. Utilizing a Mistral-7b-instruct model, FANNO efficiently produces diverse and high-quality datasets through a structured process involving document pre-screening, instruction generation, and response generation. Experiments on Open LLM Leaderboard and AlpacaEval benchmark show that the FANNO can generate high-quality data with diversity and complexity for free, comparable to human-annotated or cleaned datasets like Alpaca-GPT4-Cleaned.
## QA:
coming soon
