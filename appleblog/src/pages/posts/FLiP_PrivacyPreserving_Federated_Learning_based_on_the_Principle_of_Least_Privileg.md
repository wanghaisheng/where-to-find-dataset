---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: ShiMao Xu et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-28 11:38:10'
tags:
- all search terms
- dataset
theme: light
title: FLiP PrivacyPreserving Federated Learning based on the Principle of Least Privileg
---

# title: FLiP PrivacyPreserving Federated Learning based on the Principle of Least Privileg 
## publish date: 
**2024-10-25** 
## authors: 
  ShiMao Xu et.al. 
## paper id
2410.19548v1
## download
[2410.19548v1](http://arxiv.org/abs/2410.19548v1)
## abstracts:
Federated Learning (FL) allows users to share knowledge instead of raw data to train a model with high accuracy. Unfortunately, during the training, users lose control over the knowledge shared, which causes serious data privacy issues. We hold that users are only willing and need to share the essential knowledge to the training task to obtain the FL model with high accuracy. However, existing efforts cannot help users minimize the shared knowledge according to the user intention in the FL training procedure. This work proposes FLiP, which aims to bring the principle of least privilege (PoLP) to FL training. The key design of FLiP is applying elaborate information reduction on the training data through a local-global dataset distillation design. We measure the privacy performance through attribute inference and membership inference attacks. Extensive experiments show that FLiP strikes a good balance between model accuracy and privacy protection.
## QA:
coming soon
