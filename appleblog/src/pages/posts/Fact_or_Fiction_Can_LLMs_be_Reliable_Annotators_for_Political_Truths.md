---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Veronica Chatrath et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-11-11 11:35:20'
tags:
- all search terms
- dataset
theme: light
title: Fact or Fiction Can LLMs be Reliable Annotators for Political Truths
---

# title: Fact or Fiction Can LLMs be Reliable Annotators for Political Truths 
## publish date: 
**2024-11-08** 
## authors: 
  Veronica Chatrath et.al. 
## paper id
2411.05775v1
## download
[2411.05775v1](http://arxiv.org/abs/2411.05775v1)
## abstracts:
Political misinformation poses significant challenges to democratic processes, shaping public opinion and trust in media. Manual fact-checking methods face issues of scalability and annotator bias, while machine learning models require large, costly labelled datasets. This study investigates the use of state-of-the-art large language models (LLMs) as reliable annotators for detecting political factuality in news articles. Using open-source LLMs, we create a politically diverse dataset, labelled for bias through LLM-generated annotations. These annotations are validated by human experts and further evaluated by LLM-based judges to assess the accuracy and reliability of the annotations. Our approach offers a scalable and robust alternative to traditional fact-checking, enhancing transparency and public trust in media.
## QA:
coming soon
