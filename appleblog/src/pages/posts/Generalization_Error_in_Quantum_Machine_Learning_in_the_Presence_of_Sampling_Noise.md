---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Fangjun Hu et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-21 11:37:22'
tags:
- all search terms
- dataset
theme: light
title: Generalization Error in Quantum Machine Learning in the Presence of Sampling
  Noise
---

# title: Generalization Error in Quantum Machine Learning in the Presence of Sampling Noise 
## publish date: 
**2024-10-18** 
## authors: 
  Fangjun Hu et.al. 
## paper id
2410.14654v1
## download
[2410.14654v1](http://arxiv.org/abs/2410.14654v1)
## abstracts:
Tackling sampling noise due to finite shots of quantum measurement is an unavoidable challenge when extracting information in machine learning with physical systems. Eigentask Learning was developed in recent work as a framework for learning in the presence of sampling noise. In that work, numerical evidence was presented that extracting low-noise contributions of features can improve performance for machine learning tasks, displaying robustness to overfitting, and increasing generalization accuracy. The issue of characterizing generalization errors in situations where the training dataset is finite remains unresolved in the previous work. In this study, we use methodologies from statistical mechanics to calculate the training and generalization errors of a generic quantum machine learning system when the input training dataset and output measurement sampling shots are both finite. Our analytical findings, supported by numerical validation, offer solid justification that Eigentask Learning provides optimal learning in the sense of minimizing generalization errors.
## QA:
coming soon
