---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Chenyang Zhang et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-04-14 11:41:14'
tags:
- dataset
- all search terms
theme: light
title: Gradient Descent Robustly Learns the Intrinsic Dimension of Data in Training
  Convolutional Neural Networks
---

# title: Gradient Descent Robustly Learns the Intrinsic Dimension of Data in Training Convolutional Neural Networks 
## publish date: 
**2025-04-11** 
## authors: 
  Chenyang Zhang et.al. 
## paper id
2504.08628v1
## download
[2504.08628v1](http://arxiv.org/abs/2504.08628v1)
## abstracts:
Modern neural networks are usually highly over-parameterized. Behind the wide usage of over-parameterized networks is the belief that, if the data are simple, then the trained network will be automatically equivalent to a simple predictor. Following this intuition, many existing works have studied different notions of "ranks" of neural networks and their relation to the rank of data. In this work, we study the rank of convolutional neural networks (CNNs) trained by gradient descent, with a specific focus on the robustness of the rank to image background noises. Specifically, we point out that, when adding background noises to images, the rank of the CNN trained with gradient descent is affected far less compared with the rank of the data. We support our claim with a theoretical case study, where we consider a particular data model to characterize low-rank clean images with added background noises. We prove that CNNs trained by gradient descent can learn the intrinsic dimension of clean images, despite the presence of relatively large background noises. We also conduct experiments on synthetic and real datasets to further validate our claim.
## QA:
coming soon
