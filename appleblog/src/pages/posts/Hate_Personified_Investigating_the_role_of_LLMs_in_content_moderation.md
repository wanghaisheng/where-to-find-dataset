---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Sarah Masud et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-07 11:36:41'
tags:
- dataset
- all search terms
theme: light
title: Hate Personified Investigating the role of LLMs in content moderation
---

# title: Hate Personified Investigating the role of LLMs in content moderation 
## publish date: 
**2024-10-03** 
## authors: 
  Sarah Masud et.al. 
## paper id
2410.02657v1
## download
[2410.02657v1](http://arxiv.org/abs/2410.02657v1)
## abstracts:
For subjective tasks such as hate detection, where people perceive hate differently, the Large Language Model's (LLM) ability to represent diverse groups is unclear. By including additional context in prompts, we comprehensively analyze LLM's sensitivity to geographical priming, persona attributes, and numerical information to assess how well the needs of various groups are reflected. Our findings on two LLMs, five languages, and six datasets reveal that mimicking persona-based attributes leads to annotation variability. Meanwhile, incorporating geographical signals leads to better regional alignment. We also find that the LLMs are sensitive to numerical anchors, indicating the ability to leverage community-based flagging efforts and exposure to adversaries. Our work provides preliminary guidelines and highlights the nuances of applying LLMs in culturally sensitive cases.
## QA:
coming soon
