---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Aferdita Xhameni et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-06-23 11:47:43'
tags:
- all search terms
- dataset
theme: light
title: Highaccuracy inference using HfO_xS_yHfS_2 Memristors
---

# title: Highaccuracy inference using HfO_xS_yHfS_2 Memristors 
## publish date: 
**2025-06-20** 
## authors: 
  Aferdita Xhameni et.al. 
## paper id
2506.17174v1
## download
[2506.17174v1](http://arxiv.org/abs/2506.17174v1)
## abstracts:
We demonstrate high accuracy classification for handwritten digits from the MNIST dataset ($\sim$98.00$\%$) and RGB images from the CIFAR-10 dataset ($\sim$86.80$\%$) by using resistive memories based on a 2D van-der-Waals semiconductor: hafnium disulfide (HfS$_2$). These memories are fabricated via dry thermal oxidation, forming vertical crossbar HfO$_x$S$_y$/HfS$_2$ devices with a highly-ordered oxide-semiconductor structure. Our devices operate without electroforming or current compliance and exhibit multi-state, non-volatile resistive switching, allowing resistance to be tuned using voltage pulse trains. Using low-energy potentiation and depression pulses (0.7V-0.995V, 160ns-350ns), we achieve 31 ($\sim$5 bits) stable conductance states with high linearity, symmetry, and low variation over 100 cycles. Key performance metrics-such as weight update, quantisation, and retention-are extracted from these experimental devices. These characteristics are used to simulate neural networks with our resistive memories as weights. Neural networks are trained on state-of-the-art (SOTA) digital hardware (CUDA cores) and a baseline inference accuracy is extracted. IBM's Analog Hardware Acceleration Kit (AIHWKIT) is used to modify and remap digital weights in the pretrained network, based on the characteristics of our devices. Simulations account for factors like conductance linearity, device variation, and converter resolution. In both image recognition tasks, we demonstrate excellent performance, similar to SOTA, with only $<$0.07$\%$ and $<$1.00$\%$ difference in inference accuracy for the MNIST and CIFAR-10 datasets respectively. The forming-free, compliance-free operation, fast switching, low energy consumption, and high accuracy classification demonstrate the potential of HfO$_x$S$_y$/HfS$_2$-based resistive memories for energy-efficient neural network acceleration and neuromorphic computing.
## QA:
coming soon
