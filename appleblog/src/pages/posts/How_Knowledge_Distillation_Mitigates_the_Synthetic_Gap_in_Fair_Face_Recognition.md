---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Pedro C. Neto et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-09-02 11:33:04'
tags:
- dataset
- all search terms
theme: light
title: How Knowledge Distillation Mitigates the Synthetic Gap in Fair Face Recognition
---

# title: How Knowledge Distillation Mitigates the Synthetic Gap in Fair Face Recognition 
## publish date: 
**2024-08-30** 
## authors: 
  Pedro C. Neto et.al. 
## paper id
2408.17399v1
## download
[2408.17399v1](http://arxiv.org/abs/2408.17399v1)
## abstracts:
Leveraging the capabilities of Knowledge Distillation (KD) strategies, we devise a strategy to fight the recent retraction of face recognition datasets. Given a pretrained Teacher model trained on a real dataset, we show that carefully utilising synthetic datasets, or a mix between real and synthetic datasets to distil knowledge from this teacher to smaller students can yield surprising results. In this sense, we trained 33 different models with and without KD, on different datasets, with different architectures and losses. And our findings are consistent, using KD leads to performance gains across all ethnicities and decreased bias. In addition, it helps to mitigate the performance gap between real and synthetic datasets. This approach addresses the limitations of synthetic data training, improving both the accuracy and fairness of face recognition models.
## QA:
coming soon
