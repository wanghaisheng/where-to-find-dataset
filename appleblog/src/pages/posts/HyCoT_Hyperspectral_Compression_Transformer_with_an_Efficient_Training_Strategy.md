---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Martin Hermann Paul Fuchs et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-08-19 11:32:25'
tags:
- all search terms
- dataset
theme: light
title: HyCoT Hyperspectral Compression Transformer with an Efficient Training Strategy
---

# title: HyCoT Hyperspectral Compression Transformer with an Efficient Training Strategy 
## publish date: 
**2024-08-16** 
## authors: 
  Martin Hermann Paul Fuchs et.al. 
## paper id
2408.08700v1
## download
[2408.08700v1](http://arxiv.org/abs/2408.08700v1)
## abstracts:
The development of learning-based hyperspectral image (HSI) compression models has recently attracted significant interest. Existing models predominantly utilize convolutional filters, which capture only local dependencies. Furthermore, they often incur high training costs and exhibit substantial computational complexity. To address these limitations, in this paper we propose Hyperspectral Compression Transformer (HyCoT) that is a transformer-based autoencoder for pixelwise HSI compression. Additionally, we introduce an efficient training strategy to accelerate the training process. Experimental results on the HySpecNet-11k dataset demonstrate that HyCoT surpasses the state-of-the-art across various compression ratios by over 1 dB with significantly reduced computational requirements. Our code and pre-trained weights are publicly available at https://git.tu-berlin.de/rsim/hycot .
## QA:
coming soon
