---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Grace Proebsting et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-14 11:37:30'
tags:
- all search terms
- dataset
theme: light
title: Hypothesisonly Biases in Large Language ModelElicited Natural Language Inference
---

# title: Hypothesisonly Biases in Large Language ModelElicited Natural Language Inference 
## publish date: 
**2024-10-11** 
## authors: 
  Grace Proebsting et.al. 
## paper id
2410.08996v1
## download
[2410.08996v1](http://arxiv.org/abs/2410.08996v1)
## abstracts:
We test whether replacing crowdsource workers with LLMs to write Natural Language Inference (NLI) hypotheses similarly results in annotation artifacts. We recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 and Mistral 7b, and train hypothesis-only classifiers to determine whether LLM-elicited hypotheses contain annotation artifacts. On our LLM-elicited NLI datasets, BERT-based hypothesis-only classifiers achieve between 86-96% accuracy, indicating these datasets contain hypothesis-only artifacts. We also find frequent "give-aways" in LLM-generated hypotheses, e.g. the phrase "swimming in a pool" appears in more than 10,000 contradictions generated by GPT-4. Our analysis provides empirical evidence that well-attested biases in NLI can persist in LLM-generated data.
## QA:
coming soon
