---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Anjiang Wei et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-05-19 11:44:48'
tags:
- dataset on github
- all search terms
theme: light
title: Improving Assembly Code Performance with Large Language Models via Reinforcement
  Learning
---

# title: Improving Assembly Code Performance with Large Language Models via Reinforcement Learning 
## publish date: 
**2025-05-16** 
## authors: 
  Anjiang Wei et.al. 
## paper id
2505.11480v1
## download
[2505.11480v1](http://arxiv.org/abs/2505.11480v1)
## abstracts:
Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance.
## QA:
coming soon
