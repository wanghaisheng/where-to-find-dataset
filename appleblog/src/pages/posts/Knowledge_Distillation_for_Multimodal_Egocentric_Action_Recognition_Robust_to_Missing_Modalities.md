---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Maria Santos-Villafranca et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-04-14 11:41:16'
tags:
- dataset
- all search terms
theme: light
title: Knowledge Distillation for Multimodal Egocentric Action Recognition Robust
  to Missing Modalities
---

# title: Knowledge Distillation for Multimodal Egocentric Action Recognition Robust to Missing Modalities 
## publish date: 
**2025-04-11** 
## authors: 
  Maria Santos-Villafranca et.al. 
## paper id
2504.08578v1
## download
[2504.08578v1](http://arxiv.org/abs/2504.08578v1)
## abstracts:
Action recognition is an essential task in egocentric vision due to its wide range of applications across many fields. While deep learning methods have been proposed to address this task, most rely on a single modality, typically video. However, including additional modalities may improve the robustness of the approaches to common issues in egocentric videos, such as blurriness and occlusions. Recent efforts in multimodal egocentric action recognition often assume the availability of all modalities, leading to failures or performance drops when any modality is missing. To address this, we introduce an efficient multimodal knowledge distillation approach for egocentric action recognition that is robust to missing modalities (KARMMA) while still benefiting when multiple modalities are available. Our method focuses on resource-efficient development by leveraging pre-trained models as unimodal feature extractors in our teacher model, which distills knowledge into a much smaller and faster student model. Experiments on the Epic-Kitchens and Something-Something datasets demonstrate that our student model effectively handles missing modalities while reducing its accuracy drop in this scenario.
## QA:
coming soon
