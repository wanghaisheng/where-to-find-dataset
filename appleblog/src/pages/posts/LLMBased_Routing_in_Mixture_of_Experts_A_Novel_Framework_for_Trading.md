---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Kuan-Ming Liu et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-01-20 11:34:12'
tags:
- all search terms
- dataset
theme: light
title: LLMBased Routing in Mixture of Experts A Novel Framework for Trading
---

# title: LLMBased Routing in Mixture of Experts A Novel Framework for Trading 
## publish date: 
**2025-01-16** 
## authors: 
  Kuan-Ming Liu et.al. 
## paper id
2501.09636v2
## download
[2501.09636v2](http://arxiv.org/abs/2501.09636v2)
## abstracts:
Recent advances in deep learning and large language models (LLMs) have facilitated the deployment of the mixture-of-experts (MoE) mechanism in the stock investment domain. While these models have demonstrated promising trading performance, they are often unimodal, neglecting the wealth of information available in other modalities, such as textual data. Moreover, the traditional neural network-based router selection mechanism fails to consider contextual and real-world nuances, resulting in suboptimal expert selection. To address these limitations, we propose LLMoE, a novel framework that employs LLMs as the router within the MoE architecture. Specifically, we replace the conventional neural network-based router with LLMs, leveraging their extensive world knowledge and reasoning capabilities to select experts based on historical price data and stock news. This approach provides a more effective and interpretable selection mechanism. Our experiments on multimodal real-world stock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models and other deep neural network approaches. Additionally, the flexible architecture of LLMoE allows for easy adaptation to various downstream tasks.
## QA:
coming soon
