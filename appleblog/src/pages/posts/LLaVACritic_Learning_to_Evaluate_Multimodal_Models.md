---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Tianyi Xiong et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-07 11:36:38'
tags:
- dataset
- all search terms
theme: light
title: LLaVACritic Learning to Evaluate Multimodal Models
---

# title: LLaVACritic Learning to Evaluate Multimodal Models 
## publish date: 
**2024-10-03** 
## authors: 
  Tianyi Xiong et.al. 
## paper id
2410.02712v1
## download
[2410.02712v1](http://arxiv.org/abs/2410.02712v1)
## abstracts:
We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation criteria and scenarios. Our experiments demonstrate the model's effectiveness in two key areas: (1) LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation scores, performing on par with or surpassing GPT models on multiple evaluation benchmarks; and (2) Preference Learning, where it generates reward signals for preference learning, enhancing model alignment capabilities. This work underscores the potential of open-source LMMs in self-critique and evaluation, setting the stage for future research into scalable, superhuman alignment feedback mechanisms for LMMs.
## QA:
coming soon
