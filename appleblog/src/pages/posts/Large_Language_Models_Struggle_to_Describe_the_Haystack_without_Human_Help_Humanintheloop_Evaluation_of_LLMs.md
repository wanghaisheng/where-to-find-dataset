---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Zongxia Li et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-02-24 11:36:39'
tags:
- dataset
- all search terms
theme: light
title: Large Language Models Struggle to Describe the Haystack without Human Help
  Humanintheloop Evaluation of LLMs
---

# title: Large Language Models Struggle to Describe the Haystack without Human Help Humanintheloop Evaluation of LLMs 
## publish date: 
**2025-02-20** 
## authors: 
  Zongxia Li et.al. 
## paper id
2502.14748v1
## download
[2502.14748v1](http://arxiv.org/abs/2502.14748v1)
## abstracts:
A common use of NLP is to facilitate the understanding of large document collections, with a shift from using traditional topic models to Large Language Models. Yet the effectiveness of using LLM for large corpus understanding in real-world applications remains under-explored. This study measures the knowledge users acquire with unsupervised, supervised LLM-based exploratory approaches or traditional topic models on two datasets. While LLM-based methods generate more human-readable topics and show higher average win probabilities than traditional models for data exploration, they produce overly generic topics for domain-specific datasets that do not easily allow users to learn much about the documents. Adding human supervision to the LLM generation process improves data exploration by mitigating hallucination and over-genericity but requires greater human effort. In contrast, traditional. models like Latent Dirichlet Allocation (LDA) remain effective for exploration but are less user-friendly. We show that LLMs struggle to describe the haystack of large corpora without human help, particularly domain-specific data, and face scaling and hallucination limitations due to context length constraints. Dataset available at https://huggingface. co/datasets/zli12321/Bills.
## QA:
coming soon
