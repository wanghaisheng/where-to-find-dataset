---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Ethan Harvey et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-28 11:38:06'
tags:
- all search terms
- dataset
theme: light
title: Learning the Regularization Strength for Deep FineTuning via a DataEmphasized
  Variational Objective
---

# title: Learning the Regularization Strength for Deep FineTuning via a DataEmphasized Variational Objective 
## publish date: 
**2024-10-25** 
## authors: 
  Ethan Harvey et.al. 
## paper id
2410.19675v1
## download
[2410.19675v1](http://arxiv.org/abs/2410.19675v1)
## abstracts:
A number of popular transfer learning methods rely on grid search to select regularization hyperparameters that control over-fitting. This grid search requirement has several key disadvantages: the search is computationally expensive, requires carving out a validation set that reduces the size of available data for model training, and requires practitioners to specify candidate values. In this paper, we propose an alternative to grid search: directly learning regularization hyperparameters on the full training set via model selection techniques based on the evidence lower bound ("ELBo") objective from variational methods. For deep neural networks with millions of parameters, we specifically recommend a modified ELBo that upweights the influence of the data likelihood relative to the prior while remaining a valid bound on the evidence for Bayesian model selection. Our proposed technique overcomes all three disadvantages of grid search. We demonstrate effectiveness on image classification tasks on several datasets, yielding heldout accuracy comparable to existing approaches with far less compute time.
## QA:
coming soon
