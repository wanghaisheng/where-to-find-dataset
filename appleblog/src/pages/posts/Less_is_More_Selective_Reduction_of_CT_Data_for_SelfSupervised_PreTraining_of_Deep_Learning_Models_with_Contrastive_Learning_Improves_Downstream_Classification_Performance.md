---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Daniel Wolf et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-21 11:37:26'
tags:
- all search terms
- dataset
theme: light
title: Less is More Selective Reduction of CT Data for SelfSupervised PreTraining
  of Deep Learning Models with Contrastive Learning Improves Downstream Classification
  Performance
---

# title: Less is More Selective Reduction of CT Data for SelfSupervised PreTraining of Deep Learning Models with Contrastive Learning Improves Downstream Classification Performance 
## publish date: 
**2024-10-18** 
## authors: 
  Daniel Wolf et.al. 
## paper id
2410.14524v1
## download
[2410.14524v1](http://arxiv.org/abs/2410.14524v1)
## abstracts:
Self-supervised pre-training of deep learning models with contrastive learning is a widely used technique in image analysis. Current findings indicate a strong potential for contrastive pre-training on medical images. However, further research is necessary to incorporate the particular characteristics of these images. We hypothesize that the similarity of medical images hinders the success of contrastive learning in the medical imaging domain. To this end, we investigate different strategies based on deep embedding, information theory, and hashing in order to identify and reduce redundancy in medical pre-training datasets. The effect of these different reduction strategies on contrastive learning is evaluated on two pre-training datasets and several downstream classification tasks. In all of our experiments, dataset reduction leads to a considerable performance gain in downstream tasks, e.g., an AUC score improvement from 0.78 to 0.83 for the COVID CT Classification Grand Challenge, 0.97 to 0.98 for the OrganSMNIST Classification Challenge and 0.73 to 0.83 for a brain hemorrhage classification task. Furthermore, pre-training is up to nine times faster due to the dataset reduction. In conclusion, the proposed approach highlights the importance of dataset quality and provides a transferable approach to improve contrastive pre-training for classification downstream tasks on medical images.
## QA:
coming soon
