---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Yi-Cheng Lin et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-07-10 12:47:28'
tags:
- dataset
- all search terms
theme: light
title: Listen and Speak Fairly A Study on Semantic Gender Bias in Speech Integrated
  Large Language Models
---

# title: Listen and Speak Fairly A Study on Semantic Gender Bias in Speech Integrated Large Language Models 
## publish date: 
**2024-07-09** 
## authors: 
  Yi-Cheng Lin et.al. 
## paper id
2407.06957v1
## download
[2407.06957v1](http://arxiv.org/abs/2407.06957v1)
## abstracts:
Speech Integrated Large Language Models (SILLMs) combine large language models with speech perception to perform diverse tasks, such as emotion recognition to speaker verification, demonstrating universal audio understanding capability. However, these models may amplify biases present in training data, potentially leading to biased access to information for marginalized groups. This work introduces a curated spoken bias evaluation toolkit and corresponding dataset. We evaluate gender bias in SILLMs across four semantic-related tasks: speech-to-text translation (STT), spoken coreference resolution (SCR), spoken sentence continuation (SSC), and spoken question answering (SQA). Our analysis reveals that bias levels are language-dependent and vary with different evaluation methods. Our findings emphasize the necessity of employing multiple approaches to comprehensively assess biases in SILLMs, providing insights for developing fairer SILLM systems.
## QA:
coming soon
