---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Yixuan Wang et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-02-17 11:35:36'
tags:
- dataset
- all search terms
theme: light
title: Logical forms complement probability in understanding language model and human
  performance
---

# title: Logical forms complement probability in understanding language model and human performance 
## publish date: 
**2025-02-13** 
## authors: 
  Yixuan Wang et.al. 
## paper id
2502.09589v1
## download
[2502.09589v1](http://arxiv.org/abs/2502.09589v1)
## abstracts:
With the increasing interest in using large language models (LLMs) for planning in natural language, understanding their behaviors becomes an important research question. This work conducts a systematic investigation of LLMs' ability to perform logical reasoning in natural language. We introduce a controlled dataset of hypothetical and disjunctive syllogisms in propositional and modal logic and use it as the testbed for understanding LLM performance. Our results lead to novel insights in predicting LLM behaviors: in addition to the probability of input (Gonen et al., 2023; McCoy et al., 2024), logical forms should be considered as orthogonal factors. In addition, we show similarities and differences between the logical reasoning performances of humans and LLMs by comparing LLM and human behavioral results.
## QA:
coming soon
