---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Seunggeun Chi et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-07-22 11:32:50'
tags:
- all search terms
- dataset on github
theme: light
title: M2D2M MultiMotion Generation from Text with Discrete Diffusion Models
---

# title: M2D2M MultiMotion Generation from Text with Discrete Diffusion Models 
## publish date: 
**2024-07-19** 
## authors: 
  Seunggeun Chi et.al. 
## paper id
2407.14502v1
## download
[2407.14502v1](http://arxiv.org/abs/2407.14502v1)
## abstracts:
We introduce the Multi-Motion Discrete Diffusion Models (M2D2M), a novel approach for human motion generation from textual descriptions of multiple actions, utilizing the strengths of discrete diffusion models. This approach adeptly addresses the challenge of generating multi-motion sequences, ensuring seamless transitions of motions and coherence across a series of actions. The strength of M2D2M lies in its dynamic transition probability within the discrete diffusion model, which adapts transition probabilities based on the proximity between motion tokens, encouraging mixing between different modes. Complemented by a two-phase sampling strategy that includes independent and joint denoising steps, M2D2M effectively generates long-term, smooth, and contextually coherent human motion sequences, utilizing a model trained for single-motion generation. Extensive experiments demonstrate that M2D2M surpasses current state-of-the-art benchmarks for motion generation from text descriptions, showcasing its efficacy in interpreting language semantics and generating dynamic, realistic motions.
## QA:
coming soon
