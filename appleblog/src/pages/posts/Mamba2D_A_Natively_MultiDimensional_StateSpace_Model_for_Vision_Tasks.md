---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Enis Baty et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-12-24 01:27:46'
tags:
- all search terms
- dataset
theme: light
title: Mamba2D A Natively MultiDimensional StateSpace Model for Vision Tasks
---

# title: Mamba2D A Natively MultiDimensional StateSpace Model for Vision Tasks 
## publish date: 
**2024-12-20** 
## authors: 
  Enis Baty et.al. 
## paper id
2412.16146v1
## download
[2412.16146v1](http://arxiv.org/abs/2412.16146v1)
## abstracts:
State-Space Models (SSMs) have recently emerged as a powerful and efficient alternative to the long-standing transformer architecture. However, existing SSM conceptualizations retain deeply rooted biases from their roots in natural language processing. This constrains their ability to appropriately model the spatially-dependent characteristics of visual inputs. In this paper, we address these limitations by re-deriving modern selective state-space techniques, starting from a natively multidimensional formulation. Currently, prior works attempt to apply natively 1D SSMs to 2D data (i.e. images) by relying on arbitrary combinations of 1D scan directions to capture spatial dependencies. In contrast, Mamba2D improves upon this with a single 2D scan direction that factors in both dimensions of the input natively, effectively modelling spatial dependencies when constructing hidden states. Mamba2D shows comparable performance to prior adaptations of SSMs for vision tasks, on standard image classification evaluations with the ImageNet-1K dataset.
## QA:
coming soon
