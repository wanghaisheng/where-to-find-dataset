---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: "Martin Ki\u0161\u0161 et.al."
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-03-31 11:40:24'
tags:
- all search terms
- dataset
theme: light
title: Masked SelfSupervised PreTraining for Text Recognition Transformers on LargeScale
  Datasets
---

# title: Masked SelfSupervised PreTraining for Text Recognition Transformers on LargeScale Datasets 
## publish date: 
**2025-03-28** 
## authors: 
  Martin Kišš et.al. 
## paper id
2503.22513v1
## download
[2503.22513v1](http://arxiv.org/abs/2503.22513v1)
## abstracts:
Self-supervised learning has emerged as a powerful approach for leveraging large-scale unlabeled data to improve model performance in various domains. In this paper, we explore masked self-supervised pre-training for text recognition transformers. Specifically, we propose two modifications to the pre-training phase: progressively increasing the masking probability, and modifying the loss function to incorporate both masked and non-masked patches. We conduct extensive experiments using a dataset of 50M unlabeled text lines for pre-training and four differently sized annotated datasets for fine-tuning. Furthermore, we compare our pre-trained models against those trained with transfer learning, demonstrating the effectiveness of the self-supervised pre-training. In particular, pre-training consistently improves the character error rate of models, in some cases up to 30 % relatively. It is also on par with transfer learning but without relying on extra annotated text lines.
## QA:
coming soon
