---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Martin Tutek et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-02-24 11:36:35'
tags:
- dataset
- all search terms
theme: light
title: Measuring Faithfulness of Chains of Thought by Unlearning Reasoning Steps
---

# title: Measuring Faithfulness of Chains of Thought by Unlearning Reasoning Steps 
## publish date: 
**2025-02-20** 
## authors: 
  Martin Tutek et.al. 
## paper id
2502.14829v1
## download
[2502.14829v1](http://arxiv.org/abs/2502.14829v1)
## abstracts:
When prompted to think step-by-step, language models (LMs) produce a chain of thought (CoT), a sequence of reasoning steps that the model supposedly used to produce its prediction. However, despite much work on CoT prompting, it is unclear if CoT reasoning is faithful to the models' parameteric beliefs. We introduce a framework for measuring parametric faithfulness of generated reasoning, and propose Faithfulness by Unlearning Reasoning steps (FUR), an instance of this framework. FUR erases information contained in reasoning steps from model parameters. We perform experiments unlearning CoTs of four LMs prompted on four multi-choice question answering (MCQA) datasets. Our experiments show that FUR is frequently able to change the underlying models' prediction by unlearning key steps, indicating when a CoT is parametrically faithful. Further analysis shows that CoTs generated by models post-unlearning support different answers, hinting at a deeper effect of unlearning. Importantly, CoT steps identified as important by FUR do not align well with human notions of plausbility, emphasizing the need for specialized alignment
## QA:
coming soon
