---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Xiongtao Zhou et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-21 11:37:22'
tags:
- all search terms
- dataset
theme: light
title: MiCEval Unveiling Multimodal Chain of Thoughts Quality via Image Description
  and Reasoning Steps
---

# title: MiCEval Unveiling Multimodal Chain of Thoughts Quality via Image Description and Reasoning Steps 
## publish date: 
**2024-10-18** 
## authors: 
  Xiongtao Zhou et.al. 
## paper id
2410.14668v1
## download
[2410.14668v1](http://arxiv.org/abs/2410.14668v1)
## abstracts:
Multimodal Chain of Thought (MCoT) is a popular prompting strategy for improving the performance of multimodal large language models (MLLMs) across a range of complex reasoning tasks. Despite its popularity, there is a notable absence of automated methods for evaluating the quality of reasoning steps in MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation (MiCEval), a framework designed to assess the correctness of reasoning chains by evaluating the quality of both the description and each reasoning step. The evaluation of the description component focuses on the accuracy of the image descriptions, while the reasoning step evaluates the quality of each step as it is conditionally generated based on the preceding steps. MiCEval is built upon a fine-grained dataset with annotations that rate each step according to correctness, relevance, and informativeness. Extensive experiments on four state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more closely with human judgments compared to existing methods based on cosine similarity or fine-tuning approaches. MiCEval datasets and code can be found in https://github.com/alenai97/MiCEval.
## QA:
coming soon
