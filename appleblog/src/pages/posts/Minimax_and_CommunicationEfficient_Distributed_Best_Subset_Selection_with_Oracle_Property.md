---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Jingguo Lan et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-09-02 11:33:08'
tags:
- dataset
- all search terms
theme: light
title: Minimax and CommunicationEfficient Distributed Best Subset Selection with Oracle
  Property
---

# title: Minimax and CommunicationEfficient Distributed Best Subset Selection with Oracle Property 
## publish date: 
**2024-08-30** 
## authors: 
  Jingguo Lan et.al. 
## paper id
2408.17276v1
## download
[2408.17276v1](http://arxiv.org/abs/2408.17276v1)
## abstracts:
The explosion of large-scale data in fields such as finance, e-commerce, and social media has outstripped the processing capabilities of single-machine systems, driving the need for distributed statistical inference methods. Traditional approaches to distributed inference often struggle with achieving true sparsity in high-dimensional datasets and involve high computational costs. We propose a novel, two-stage, distributed best subset selection algorithm to address these issues. Our approach starts by efficiently estimating the active set while adhering to the $\ell_0$ norm-constrained surrogate likelihood function, effectively reducing dimensionality and isolating key variables. A refined estimation within the active set follows, ensuring sparse estimates and matching the minimax $\ell_2$ error bound. We introduce a new splicing technique for adaptive parameter selection to tackle subproblems under $\ell_0$ constraints and a Generalized Information Criterion (GIC). Our theoretical and numerical studies show that the proposed algorithm correctly finds the true sparsity pattern, has the oracle property, and greatly lowers communication costs. This is a big step forward in distributed sparse estimation.
## QA:
coming soon
