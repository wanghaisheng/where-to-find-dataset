---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Bruno Bianchi et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-05-19 11:44:47'
tags:
- dataset on github
- all search terms
theme: light
title: Modeling cognitive processes of natural reading with transformerbased Language
  Models
---

# title: Modeling cognitive processes of natural reading with transformerbased Language Models 
## publish date: 
**2025-05-16** 
## authors: 
  Bruno Bianchi et.al. 
## paper id
2505.11485v1
## download
[2505.11485v1](http://arxiv.org/abs/2505.11485v1)
## abstracts:
Recent advances in Natural Language Processing (NLP) have led to the development of highly sophisticated language models for text generation. In parallel, neuroscience has increasingly employed these models to explore cognitive processes involved in language comprehension. Previous research has shown that models such as N-grams and LSTM networks can partially account for predictability effects in explaining eye movement behaviors, specifically Gaze Duration, during reading. In this study, we extend these findings by evaluating transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate this relationship. Our results indicate that these architectures outperform earlier models in explaining the variance in Gaze Durations recorded from Rioplantense Spanish readers. However, similar to previous studies, these models still fail to account for the entirety of the variance captured by human predictability. These findings suggest that, despite their advancements, state-of-the-art language models continue to predict language in ways that differ from human readers.
## QA:
coming soon
