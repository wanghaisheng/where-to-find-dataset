---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Wissam Antoun et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-04-14 11:41:11'
tags:
- dataset
- all search terms
theme: light
title: ModernBERT or DeBERTaV3 Examining Architecture and Data Influence on Transformer
  Encoder Models Performance
---

# title: ModernBERT or DeBERTaV3 Examining Architecture and Data Influence on Transformer Encoder Models Performance 
## publish date: 
**2025-04-11** 
## authors: 
  Wissam Antoun et.al. 
## paper id
2504.08716v1
## download
[2504.08716v1](http://arxiv.org/abs/2504.08716v1)
## abstracts:
Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.
## QA:
coming soon
