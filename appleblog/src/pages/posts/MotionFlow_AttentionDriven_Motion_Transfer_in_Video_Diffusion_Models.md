---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Tuna Han Salih Meral et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-12-09 11:41:03'
tags:
- all search terms
- dataset on github
theme: light
title: MotionFlow AttentionDriven Motion Transfer in Video Diffusion Models
---

# title: MotionFlow AttentionDriven Motion Transfer in Video Diffusion Models 
## publish date: 
**2024-12-06** 
## authors: 
  Tuna Han Salih Meral et.al. 
## paper id
2412.05275v1
## download
[2412.05275v1](http://arxiv.org/abs/2412.05275v1)
## abstracts:
Text-to-video models have demonstrated impressive capabilities in producing diverse and captivating video content, showcasing a notable advancement in generative AI. However, these models generally lack fine-grained control over motion patterns, limiting their practical applicability. We introduce MotionFlow, a novel framework designed for motion transfer in video diffusion models. Our method utilizes cross-attention maps to accurately capture and manipulate spatial and temporal dynamics, enabling seamless motion transfers across various contexts. Our approach does not require training and works on test-time by leveraging the inherent capabilities of pre-trained video diffusion models. In contrast to traditional approaches, which struggle with comprehensive scene changes while maintaining consistent motion, MotionFlow successfully handles such complex transformations through its attention-based mechanism. Our qualitative and quantitative experiments demonstrate that MotionFlow significantly outperforms existing models in both fidelity and versatility even during drastic scene alterations.
## QA:
coming soon
