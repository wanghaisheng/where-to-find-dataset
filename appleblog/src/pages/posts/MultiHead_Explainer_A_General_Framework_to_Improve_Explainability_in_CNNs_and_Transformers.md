---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Bohang Sun et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-01-06 11:37:03'
tags:
- all search terms
- dataset
theme: light
title: MultiHead Explainer A General Framework to Improve Explainability in CNNs and
  Transformers
---

# title: MultiHead Explainer A General Framework to Improve Explainability in CNNs and Transformers 
## publish date: 
**2025-01-02** 
## authors: 
  Bohang Sun et.al. 
## paper id
2501.01311v1
## download
[2501.01311v1](http://arxiv.org/abs/2501.01311v1)
## abstracts:
In this study, we introduce the Multi-Head Explainer (MHEX), a versatile and modular framework that enhances both the explainability and accuracy of Convolutional Neural Networks (CNNs) and Transformer-based models. MHEX consists of three core components: an Attention Gate that dynamically highlights task-relevant features, Deep Supervision that guides early layers to capture fine-grained details pertinent to the target class, and an Equivalent Matrix that unifies refined local and global representations to generate comprehensive saliency maps. Our approach demonstrates superior compatibility, enabling effortless integration into existing residual networks like ResNet and Transformer architectures such as BERT with minimal modifications. Extensive experiments on benchmark datasets in medical imaging and text classification show that MHEX not only improves classification accuracy but also produces highly interpretable and detailed saliency scores.
## QA:
coming soon
