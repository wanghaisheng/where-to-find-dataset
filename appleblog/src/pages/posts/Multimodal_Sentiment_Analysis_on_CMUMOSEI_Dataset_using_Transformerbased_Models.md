---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Jugal Gajjar et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-05-12 11:43:55'
tags:
- dataset
- all search terms
theme: light
title: Multimodal Sentiment Analysis on CMUMOSEI Dataset using Transformerbased Models
---

# title: Multimodal Sentiment Analysis on CMUMOSEI Dataset using Transformerbased Models 
## publish date: 
**2025-05-09** 
## authors: 
  Jugal Gajjar et.al. 
## paper id
2505.06110v1
## download
[2505.06110v1](http://arxiv.org/abs/2505.06110v1)
## abstracts:
This project performs multimodal sentiment analysis using the CMU-MOSEI dataset, using transformer-based models with early fusion to integrate text, audio, and visual modalities. We employ BERT-based encoders for each modality, extracting embeddings that are concatenated before classification. The model achieves strong performance, with 97.87\% 7-class accuracy and a 0.9682 F1-score on the test set, demonstrating the effectiveness of early fusion in capturing cross-modal interactions. The training utilized Adam optimization (lr=1e-4), dropout (0.3), and early stopping to ensure generalization and robustness. Results highlight the superiority of transformer architectures in modeling multimodal sentiment, with a low MAE (0.1060) indicating precise sentiment intensity prediction. Future work may compare fusion strategies or enhance interpretability. This approach utilizes multimodal learning by effectively combining linguistic, acoustic, and visual cues for sentiment analysis.
## QA:
coming soon
