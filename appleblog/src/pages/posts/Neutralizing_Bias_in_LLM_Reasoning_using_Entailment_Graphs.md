---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Liang Cheng et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-03-17 11:38:44'
tags:
- dataset
- all search terms
theme: light
title: Neutralizing Bias in LLM Reasoning using Entailment Graphs
---

# title: Neutralizing Bias in LLM Reasoning using Entailment Graphs 
## publish date: 
**2025-03-14** 
## authors: 
  Liang Cheng et.al. 
## paper id
2503.11614v1
## download
[2503.11614v1](http://arxiv.org/abs/2503.11614v1)
## abstracts:
LLMs are often claimed to be capable of Natural Language Inference (NLI), which is widely regarded as a cornerstone of more complex forms of reasoning. However, recent works show that LLMs still suffer from hallucinations in NLI due to attestation bias, where LLMs overly rely on propositional memory to build shortcuts. To solve the issue, we design an unsupervised framework to construct counterfactual reasoning data and fine-tune LLMs to reduce attestation bias. To measure bias reduction, we build bias-adversarial variants of NLI datasets with randomly replaced predicates in premises while keeping hypotheses unchanged. Extensive evaluations show that our framework can significantly reduce hallucinations from attestation bias. Then, we further evaluate LLMs fine-tuned with our framework on original NLI datasets and their bias-neutralized versions, where original entities are replaced with randomly sampled ones. Extensive results show that our framework consistently improves inferential performance on both original and bias-neutralized NLI datasets.
## QA:
coming soon
