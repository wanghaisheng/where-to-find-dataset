---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Pushkar Mishra et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-03-10 11:31:49'
tags:
- dataset
- all search terms
theme: light
title: Nuanced Safety for Generative AI How Demographics Shape Responsiveness to Severity
---

# title: Nuanced Safety for Generative AI How Demographics Shape Responsiveness to Severity 
## publish date: 
**2025-03-07** 
## authors: 
  Pushkar Mishra et.al. 
## paper id
2503.05609v1
## download
[2503.05609v1](http://arxiv.org/abs/2503.05609v1)
## abstracts:
Ensuring safety of Generative AI requires a nuanced understanding of pluralistic viewpoints. In this paper, we introduce a novel data-driven approach for calibrating granular ratings in pluralistic datasets. Specifically, we address the challenge of interpreting responses of a diverse population to safety expressed via ordinal scales (e.g., Likert scale). We distill non-parametric responsiveness metrics that quantify the consistency of raters in scoring the varying levels of the severity of safety violations. Using safety evaluation of AI-generated content as a case study, we investigate how raters from different demographic groups (age, gender, ethnicity) use an ordinal scale to express their perception of the severity of violations in a pluralistic safety dataset. We apply our metrics across violation types, demonstrating their utility in extracting nuanced insights that are crucial for developing reliable AI systems in a multi-cultural contexts. We show that our approach offers improved capabilities for prioritizing safety concerns by capturing nuanced viewpoints across different demographic groups, hence improving the reliability of pluralistic data collection and in turn contributing to more robust AI evaluations.
## QA:
coming soon
