---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Pascal Pernot et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-08-26 11:33:40'
tags:
- dataset
- all search terms
theme: light
title: On the good reliability of an intervalbased metric to validate prediction uncertainty
  for machine learning regression tasks
---

# title: On the good reliability of an intervalbased metric to validate prediction uncertainty for machine learning regression tasks 
## publish date: 
**2024-08-23** 
## authors: 
  Pascal Pernot et.al. 
## paper id
2408.13089v1
## download
[2408.13089v1](http://arxiv.org/abs/2408.13089v1)
## abstracts:
This short study presents an opportunistic approach to a (more) reliable validation method for prediction uncertainty average calibration. Considering that variance-based calibration metrics (ZMS, NLL, RCE...) are quite sensitive to the presence of heavy tails in the uncertainty and error distributions, a shift is proposed to an interval-based metric, the Prediction Interval Coverage Probability (PICP). It is shown on a large ensemble of molecular properties datasets that (1) sets of z-scores are well represented by Student's-$t(\nu)$ distributions, $\nu$ being the number of degrees of freedom; (2) accurate estimation of 95 $\%$ prediction intervals can be obtained by the simple $2\sigma$ rule for $\nu>3$; and (3) the resulting PICPs are more quickly and reliably tested than variance-based calibration metrics. Overall, this method enables to test 20 $\%$ more datasets than ZMS testing. Conditional calibration is also assessed using the PICP approach.
## QA:
coming soon
