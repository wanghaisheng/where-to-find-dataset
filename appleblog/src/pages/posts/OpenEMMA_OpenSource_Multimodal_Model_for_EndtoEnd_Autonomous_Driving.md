---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Shuo Xing et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-12-23 11:35:34'
tags:
- all search terms
- dataset
theme: light
title: OpenEMMA OpenSource Multimodal Model for EndtoEnd Autonomous Driving
---

# title: OpenEMMA OpenSource Multimodal Model for EndtoEnd Autonomous Driving 
## publish date: 
**2024-12-19** 
## authors: 
  Shuo Xing et.al. 
## paper id
2412.15208v1
## download
[2412.15208v1](http://arxiv.org/abs/2412.15208v1)
## abstracts:
Since the advent of Multimodal Large Language Models (MLLMs), they have made a significant impact across a wide range of real-world applications, particularly in Autonomous Driving (AD). Their ability to process complex visual data and reason about intricate driving scenarios has paved the way for a new paradigm in end-to-end AD systems. However, the progress of developing end-to-end models for AD has been slow, as existing fine-tuning methods demand substantial resources, including extensive computational power, large-scale datasets, and significant funding. Drawing inspiration from recent advancements in inference computing, we propose OpenEMMA, an open-source end-to-end framework based on MLLMs. By incorporating the Chain-of-Thought reasoning process, OpenEMMA achieves significant improvements compared to the baseline when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates effectiveness, generalizability, and robustness across a variety of challenging driving scenarios, offering a more efficient and effective approach to autonomous driving. We release all the codes in https://github.com/taco-group/OpenEMMA.
## QA:
coming soon
