---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Xiaoman Zhang et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-11-25 11:39:17'
tags:
- all search terms
- dataset on github
theme: light
title: ReXrank A Public Leaderboard for AIPowered Radiology Report Generation
---

# title: ReXrank A Public Leaderboard for AIPowered Radiology Report Generation 
## publish date: 
**2024-11-22** 
## authors: 
  Xiaoman Zhang et.al. 
## paper id
2411.15122v1
## download
[2411.15122v1](http://arxiv.org/abs/2411.15122v1)
## abstracts:
AI-driven models have demonstrated significant potential in automating radiology report generation for chest X-rays. However, there is no standardized benchmark for objectively evaluating their performance. To address this, we present ReXrank, https://rexrank.ai, a public leaderboard and challenge for assessing AI-powered radiology report generation. Our framework incorporates ReXGradient, the largest test dataset consisting of 10,000 studies, and three public datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) for report generation assessment. ReXrank employs 8 evaluation metrics and separately assesses models capable of generating only findings sections and those providing both findings and impressions sections. By providing this standardized evaluation framework, ReXrank enables meaningful comparisons of model performance and offers crucial insights into their robustness across diverse clinical settings. Beyond its current focus on chest X-rays, ReXrank's framework sets the stage for comprehensive evaluation of automated reporting across the full spectrum of medical imaging.
## QA:
coming soon
