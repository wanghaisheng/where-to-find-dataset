---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Jihwan Jeong et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-06-09 11:46:07'
tags:
- all search terms
- dataset on github
theme: light
title: ReflectthenPlan Offline ModelBased Planning through a Doubly Bayesian Lens
---

# title: ReflectthenPlan Offline ModelBased Planning through a Doubly Bayesian Lens 
## publish date: 
**2025-06-06** 
## authors: 
  Jihwan Jeong et.al. 
## paper id
2506.06261v1
## download
[2506.06261v1](http://arxiv.org/abs/2506.06261v1)
## abstracts:
Offline reinforcement learning (RL) is crucial when online exploration is costly or unsafe but often struggles with high epistemic uncertainty due to limited data. Existing methods rely on fixed conservative policies, restricting adaptivity and generalization. To address this, we propose Reflect-then-Plan (RefPlan), a novel doubly Bayesian offline model-based (MB) planning approach. RefPlan unifies uncertainty modeling and MB planning by recasting planning as Bayesian posterior estimation. At deployment, it updates a belief over environment dynamics using real-time observations, incorporating uncertainty into MB planning via marginalization. Empirical results on standard benchmarks show that RefPlan significantly improves the performance of conservative offline RL policies. In particular, RefPlan maintains robust performance under high epistemic uncertainty and limited data, while demonstrating resilience to changing environment dynamics, improving the flexibility, generalizability, and robustness of offline-learned policies.
## QA:
coming soon
