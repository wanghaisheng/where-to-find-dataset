---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Yucheng Zhou et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-28 11:38:04'
tags:
- all search terms
- dataset
theme: light
title: Rethinking Visual Dependency in LongContext Reasoning for Large VisionLanguage
  Models
---

# title: Rethinking Visual Dependency in LongContext Reasoning for Large VisionLanguage Models 
## publish date: 
**2024-10-25** 
## authors: 
  Yucheng Zhou et.al. 
## paper id
2410.19732v1
## download
[2410.19732v1](http://arxiv.org/abs/2410.19732v1)
## abstracts:
Large Vision-Language Models (LVLMs) excel in cross-model tasks but experience performance declines in long-context reasoning due to overreliance on textual information and reduced visual dependency. In this study, we empirically analyze LVLMs in long-context reasoning, revealing that increased context length leads to a higher dependence on language at the expense of visual dependency. To address this issue, we propose a novel training-free context pruning method that selectively removes less critical textual information. Our approach enhances visual dependency and reduces textual noise, thereby improving LVLM performance in long-context reasoning. We validate our method by constructing a long-context dataset, demonstrating its effectiveness across various LVLMs. Moreover, further analysis confirms the robustness of different token pruning strategies and preliminary explores scaling laws between pruning rates and context length.
## QA:
coming soon
