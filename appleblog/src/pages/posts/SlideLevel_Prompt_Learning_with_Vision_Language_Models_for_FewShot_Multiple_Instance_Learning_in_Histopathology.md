---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Devavrat Tomar et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-03-24 11:39:30'
tags:
- dataset
- all search terms
theme: light
title: SlideLevel Prompt Learning with Vision Language Models for FewShot Multiple
  Instance Learning in Histopathology
---

# title: SlideLevel Prompt Learning with Vision Language Models for FewShot Multiple Instance Learning in Histopathology 
## publish date: 
**2025-03-21** 
## authors: 
  Devavrat Tomar et.al. 
## paper id
2503.17238v1
## download
[2503.17238v1](http://arxiv.org/abs/2503.17238v1)
## abstracts:
In this paper, we address the challenge of few-shot classification in histopathology whole slide images (WSIs) by utilizing foundational vision-language models (VLMs) and slide-level prompt learning. Given the gigapixel scale of WSIs, conventional multiple instance learning (MIL) methods rely on aggregation functions to derive slide-level (bag-level) predictions from patch representations, which require extensive bag-level labels for training. In contrast, VLM-based approaches excel at aligning visual embeddings of patches with candidate class text prompts but lack essential pathological prior knowledge. Our method distinguishes itself by utilizing pathological prior knowledge from language models to identify crucial local tissue types (patches) for WSI classification, integrating this within a VLM-based MIL framework. Our approach effectively aligns patch images with tissue types, and we fine-tune our model via prompt learning using only a few labeled WSIs per category. Experimentation on real-world pathological WSI datasets and ablation studies highlight our method's superior performance over existing MIL- and VLM-based methods in few-shot WSI classification tasks. Our code is publicly available at https://github.com/LTS5/SLIP.
## QA:
coming soon
