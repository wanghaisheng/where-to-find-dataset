---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Philipp Guevorguian et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-07-29 11:32:42'
tags:
- dataset on github
- all search terms
theme: light
title: Small Molecule Optimization with Large Language Models
---

# title: Small Molecule Optimization with Large Language Models 
## publish date: 
**2024-07-26** 
## authors: 
  Philipp Guevorguian et.al. 
## paper id
2407.18897v1
## download
[2407.18897v1](http://arxiv.org/abs/2407.18897v1)
## abstracts:
Recent advancements in large language models have opened new possibilities for generative molecular drug design. We present Chemlactica and Chemma, two language models fine-tuned on a novel corpus of 110M molecules with computed properties, totaling 40B tokens. These models demonstrate strong performance in generating molecules with specified properties and predicting new molecular characteristics from limited samples. We introduce a novel optimization algorithm that leverages our language models to optimize molecules for arbitrary properties given limited access to a black box oracle. Our approach combines ideas from genetic algorithms, rejection sampling, and prompt optimization. It achieves state-of-the-art performance on multiple molecular optimization benchmarks, including an 8% improvement on Practical Molecular Optimization compared to previous methods. We publicly release the training corpus, the language models and the optimization algorithm.
## QA:
coming soon
