---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Hyesu Lim et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-12-09 11:41:02'
tags:
- all search terms
- dataset on github
theme: light
title: Sparse autoencoders reveal selective remapping of visual concepts during adaptation
---

# title: Sparse autoencoders reveal selective remapping of visual concepts during adaptation 
## publish date: 
**2024-12-06** 
## authors: 
  Hyesu Lim et.al. 
## paper id
2412.05276v1
## download
[2412.05276v1](http://arxiv.org/abs/2412.05276v1)
## abstracts:
Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named PatchSAE, to extract interpretable concepts at granular levels (e.g. shape, color, or semantics of an object) and their patch-wise spatial attributions. We explore how these concepts influence the model output in downstream image classification tasks and investigate how recent state-of-the-art prompt-based adaptation techniques change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model. This work provides a concrete framework to train and use SAEs for Vision Transformers and provides insights into explaining adaptation mechanisms.
## QA:
coming soon
