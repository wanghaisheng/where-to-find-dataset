---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Boya Zhang et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-03-03 11:38:11'
tags:
- dataset
- all search terms
theme: light
title: The Role of Tactile Sensing for Learning Reach and Grasp
---

# title: The Role of Tactile Sensing for Learning Reach and Grasp 
## publish date: 
**2025-02-27** 
## authors: 
  Boya Zhang et.al. 
## paper id
2502.20367v1
## download
[2502.20367v1](http://arxiv.org/abs/2502.20367v1)
## abstracts:
Stable and robust robotic grasping is essential for current and future robot applications. In recent works, the use of large datasets and supervised learning has enhanced speed and precision in antipodal grasping. However, these methods struggle with perception and calibration errors due to large planning horizons. To obtain more robust and reactive grasping motions, leveraging reinforcement learning combined with tactile sensing is a promising direction. Yet, there is no systematic evaluation of how the complexity of force-based tactile sensing affects the learning behavior for grasping tasks. This paper compares various tactile and environmental setups using two model-free reinforcement learning approaches for antipodal grasping. Our findings suggest that under imperfect visual perception, various tactile features improve learning outcomes, while complex tactile inputs complicate training.
## QA:
coming soon
