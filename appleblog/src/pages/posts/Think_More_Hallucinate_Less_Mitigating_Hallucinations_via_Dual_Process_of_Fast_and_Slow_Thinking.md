---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Xiaoxue Cheng et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-01-06 11:37:03'
tags:
- all search terms
- dataset
theme: light
title: Think More Hallucinate Less Mitigating Hallucinations via Dual Process of Fast
  and Slow Thinking
---

# title: Think More Hallucinate Less Mitigating Hallucinations via Dual Process of Fast and Slow Thinking 
## publish date: 
**2025-01-02** 
## authors: 
  Xiaoxue Cheng et.al. 
## paper id
2501.01306v2
## download
[2501.01306v2](http://arxiv.org/abs/2501.01306v2)
## abstracts:
Large language models (LLMs) demonstrate exceptional capabilities, yet still face the hallucination issue. Typical text generation approaches adopt an auto-regressive generation without deliberate reasoning, which often results in untrustworthy and factually inaccurate responses. In this paper, we propose HaluSearch, a novel framework that incorporates tree search-based algorithms (e.g. MCTS) to enable an explicit slow thinking generation process for mitigating hallucinations of LLMs during inference. Specifically, HaluSearch frames text generation as a step-by-step reasoning process, using a self-evaluation reward model to score each generation step and guide the tree search towards the most reliable generation pathway for fully exploiting the internal knowledge of LLMs. To balance efficiency and quality, we introduce a hierarchical thinking system switch mechanism inspired by the dual process theory in cognitive science, which dynamically alternates between fast and slow thinking modes at both the instance and step levels, adapting to the complexity of questions and reasoning states. We conduct extensive experiments on both English and Chinese datasets and the results show that our approach significantly outperforms baseline approaches.
## QA:
coming soon
