---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Daniele Paliotta et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-03-03 11:38:14'
tags:
- dataset
- all search terms
theme: light
title: Thinking Slow Fast Scaling Inference Compute with Distilled Reasoners
---

# title: Thinking Slow Fast Scaling Inference Compute with Distilled Reasoners 
## publish date: 
**2025-02-27** 
## authors: 
  Daniele Paliotta et.al. 
## paper id
2502.20339v1
## download
[2502.20339v1](http://arxiv.org/abs/2502.20339v1)
## abstracts:
Recent advancements have demonstrated that the performance of large language models (LLMs) can be significantly enhanced by scaling computational resources at test time. A common strategy involves generating multiple Chain-of-Thought (CoT) trajectories and aggregating their outputs through various selection mechanisms. This raises a fundamental question: can models with lower complexity leverage their superior generation throughput to outperform similarly sized Transformers for a fixed computational budget? To address this question and overcome the lack of strong subquadratic reasoners, we distill pure and hybrid Mamba models from pretrained Transformers. Trained on only 8 billion tokens, our distilled models show strong performance and scaling on mathematical reasoning datasets while being much faster at inference for large batches and long sequences. Despite the zero-shot performance hit due to distillation, both pure and hybrid Mamba models can scale their coverage and accuracy performance past their Transformer teacher models under fixed time budgets, opening a new direction for scaling inference compute.
## QA:
coming soon
