---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Ranindya Paramitha et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-06-16 11:46:08'
tags:
- all search terms
- dataset
theme: light
title: Todays Cat Is Tomorrows Dog Accounting for TimeBased Changes in the Labels
  of ML Vulnerability Detection Approaches
---

# title: Todays Cat Is Tomorrows Dog Accounting for TimeBased Changes in the Labels of ML Vulnerability Detection Approaches 
## publish date: 
**2025-06-13** 
## authors: 
  Ranindya Paramitha et.al. 
## paper id
2506.11939v1
## download
[2506.11939v1](http://arxiv.org/abs/2506.11939v1)
## abstracts:
Vulnerability datasets used for ML testing implicitly contain retrospective information. When tested on the field, one can only use the labels available at the time of training and testing (e.g. seen and assumed negatives). As vulnerabilities are discovered across calendar time, labels change and past performance is not necessarily aligned with future performance. Past works only considered the slices of the whole history (e.g. DiverseVUl) or individual differences between releases (e.g. Jimenez et al. ESEC/FSE 2019). Such approaches are either too optimistic in training (e.g. the whole history) or too conservative (e.g. consecutive releases). We propose a method to restructure a dataset into a series of datasets in which both training and testing labels change to account for the knowledge available at the time. If the model is actually learning, it should improve its performance over time as more data becomes available and data becomes more stable, an effect that can be checked with the Mann-Kendall test. We validate our methodology for vulnerability detection with 4 time-based datasets (3 projects from BigVul dataset + Vuldeepecker's NVD) and 5 ML models (Code2Vec, CodeBERT, LineVul, ReGVD, and Vuldeepecker). In contrast to the intuitive expectation (more retrospective information, better performance), the trend results show that performance changes inconsistently across the years, showing that most models are not learning.
## QA:
coming soon
