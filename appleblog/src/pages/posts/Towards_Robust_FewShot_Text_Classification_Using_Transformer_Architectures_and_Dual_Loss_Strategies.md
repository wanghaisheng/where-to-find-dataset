---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Xu Han et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-05-12 11:43:53'
tags:
- dataset
- all search terms
theme: light
title: Towards Robust FewShot Text Classification Using Transformer Architectures
  and Dual Loss Strategies
---

# title: Towards Robust FewShot Text Classification Using Transformer Architectures and Dual Loss Strategies 
## publish date: 
**2025-05-09** 
## authors: 
  Xu Han et.al. 
## paper id
2505.06145v1
## download
[2505.06145v1](http://arxiv.org/abs/2505.06145v1)
## abstracts:
Few-shot text classification has important application value in low-resource environments. This paper proposes a strategy that combines adaptive fine-tuning, contrastive learning, and regularization optimization to improve the classification performance of Transformer-based models. Experiments on the FewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform well in few-shot tasks, especially in the 5-shot setting, which can more effectively capture text features and improve classification accuracy. The experiment also found that there are significant differences in the classification difficulty of different relationship categories. Some categories have fuzzy semantic boundaries or complex feature distributions, making it difficult for the standard cross entropy loss to learn the discriminative information required to distinguish categories. By introducing contrastive loss and regularization loss, the generalization ability of the model is enhanced, effectively alleviating the overfitting problem in few-shot environments. In addition, the research results show that the use of Transformer models or generative architectures with stronger self-attention mechanisms can help improve the stability and accuracy of few-shot classification.
## QA:
coming soon
