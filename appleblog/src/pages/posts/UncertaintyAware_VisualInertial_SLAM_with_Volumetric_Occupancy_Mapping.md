---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Jaehyung Jung et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-09-23 11:35:17'
tags:
- dataset
- all search terms
theme: light
title: UncertaintyAware VisualInertial SLAM with Volumetric Occupancy Mapping
---

# title: UncertaintyAware VisualInertial SLAM with Volumetric Occupancy Mapping 
## publish date: 
**2024-09-18** 
## authors: 
  Jaehyung Jung et.al. 
## paper id
2409.12051v1
## download
[2409.12051v1](http://arxiv.org/abs/2409.12051v1)
## abstracts:
We propose visual-inertial simultaneous localization and mapping that tightly couples sparse reprojection errors, inertial measurement unit pre-integrals, and relative pose factors with dense volumetric occupancy mapping. Hereby depth predictions from a deep neural network are fused in a fully probabilistic manner. Specifically, our method is rigorously uncertainty-aware: first, we use depth and uncertainty predictions from a deep network not only from the robot's stereo rig, but we further probabilistically fuse motion stereo that provides depth information across a range of baselines, therefore drastically increasing mapping accuracy. Next, predicted and fused depth uncertainty propagates not only into occupancy probabilities but also into alignment factors between generated dense submaps that enter the probabilistic nonlinear least squares estimator. This submap representation offers globally consistent geometry at scale. Our method is thoroughly evaluated in two benchmark datasets, resulting in localization and mapping accuracy that exceeds the state of the art, while simultaneously offering volumetric occupancy directly usable for downstream robotic planning and control in real-time.
## QA:
coming soon
