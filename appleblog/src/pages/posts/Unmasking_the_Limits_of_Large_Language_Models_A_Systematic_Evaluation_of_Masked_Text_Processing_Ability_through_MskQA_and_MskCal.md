---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Fuka Matsuzaki et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-11-11 11:35:28'
tags:
- all search terms
- dataset
theme: light
title: Unmasking the Limits of Large Language Models A Systematic Evaluation of Masked
  Text Processing Ability through MskQA and MskCal
---

# title: Unmasking the Limits of Large Language Models A Systematic Evaluation of Masked Text Processing Ability through MskQA and MskCal 
## publish date: 
**2024-11-08** 
## authors: 
  Fuka Matsuzaki et.al. 
## paper id
2411.05665v1
## download
[2411.05665v1](http://arxiv.org/abs/2411.05665v1)
## abstracts:
This paper sheds light on the limitations of Large Language Models (LLMs) by rigorously evaluating their ability to process masked text. We introduce two novel tasks: MskQA, measuring reasoning on masked question-answering datasets like RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic problems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some resilience to masked text, their performance is highly contingent on masking rates and semantic cues. Specifically, "solid masking," where semantic clues are entirely absent, leads to a significant performance drop compared to "partial lifting," where some semantic information is retained, indicating LLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently outperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to handle numerical reasoning with masked text. This underscores the crucial role of semantic cues in the reasoning process of LLMs. Our study illuminates the interplay between background knowledge and reasoning ability in masked text processing, paving the way for a deeper understanding of LLM capabilities and limitations, and highlighting the need for more robust evaluation methods to accurately assess their true comprehension abilities.
## QA:
coming soon
