---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Austin T. Wang et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-01-06 11:37:00'
tags:
- all search terms
- dataset
theme: light
title: ViGiL3D A Linguistically Diverse Dataset for 3D Visual Grounding
---

# title: ViGiL3D A Linguistically Diverse Dataset for 3D Visual Grounding 
## publish date: 
**2025-01-02** 
## authors: 
  Austin T. Wang et.al. 
## paper id
2501.01366v1
## download
[2501.01366v1](http://arxiv.org/abs/2501.01366v1)
## abstracts:
3D visual grounding (3DVG) involves localizing entities in a 3D scene referred to by natural language text. Such models are useful for embodied AI and scene retrieval applications, which involve searching for objects or patterns using natural language descriptions. While recent works have focused on LLM-based scaling of 3DVG datasets, these datasets do not capture the full range of potential prompts which could be specified in the English language. To ensure that we are scaling up and testing against a useful and representative set of prompts, we propose a framework for linguistically analyzing 3DVG prompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a diagnostic dataset for evaluating visual grounding methods against a diverse set of language patterns. We evaluate existing open-vocabulary 3DVG methods to demonstrate that these methods are not yet proficient in understanding and identifying the targets of more challenging, out-of-distribution prompts, toward real-world applications.
## QA:
coming soon
