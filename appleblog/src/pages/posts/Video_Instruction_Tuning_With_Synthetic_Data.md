---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Yuanhan Zhang et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2024-10-07 11:36:38'
tags:
- dataset
- all search terms
theme: light
title: Video Instruction Tuning With Synthetic Data
---

# title: Video Instruction Tuning With Synthetic Data 
## publish date: 
**2024-10-03** 
## authors: 
  Yuanhan Zhang et.al. 
## paper id
2410.02713v2
## download
[2410.02713v2](http://arxiv.org/abs/2410.02713v2)
## abstracts:
The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we propose an alternative approach by creating a high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this dataset, in combination with existing visual instruction tuning data, we introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that LLaVA-Video achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints.
## QA:
coming soon
