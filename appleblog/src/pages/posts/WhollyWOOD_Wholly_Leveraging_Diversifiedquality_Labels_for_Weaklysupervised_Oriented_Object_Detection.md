---
author: wanghaisheng
cover:
  alt: cover
  square: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
  url: https://www.apple.com.cn/newsroom/images/product/homepod/standard/Apple-HomePod-hero-230118_big.jpg.large_2x.jpg
description: ''
featured: true
keywords: key1, key2, key3
layout: ../../layouts/MarkdownPost.astro
meta:
- content: Yi Yu et.al.
  name: author
- content: key3, key4
  name: keywords
pubDate: '2025-02-17 11:35:43'
tags:
- dataset
- all search terms
theme: light
title: WhollyWOOD Wholly Leveraging Diversifiedquality Labels for Weaklysupervised
  Oriented Object Detection
---

# title: WhollyWOOD Wholly Leveraging Diversifiedquality Labels for Weaklysupervised Oriented Object Detection 
## publish date: 
**2025-02-13** 
## authors: 
  Yi Yu et.al. 
## paper id
2502.09471v1
## download
[2502.09471v1](http://arxiv.org/abs/2502.09471v1)
## abstracts:
Accurately estimating the orientation of visual objects with compact rotated bounding boxes (RBoxes) has become a prominent demand, which challenges existing object detection paradigms that only use horizontal bounding boxes (HBoxes). To equip the detectors with orientation awareness, supervised regression/classification modules have been introduced at the high cost of rotation annotation. Meanwhile, some existing datasets with oriented objects are already annotated with horizontal boxes or even single points. It becomes attractive yet remains open for effectively utilizing weaker single point and horizontal annotations to train an oriented object detector (OOD). We develop Wholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging various labeling forms (Points, HBoxes, RBoxes, and their combination) in a unified fashion. By only using HBox for training, our Wholly-WOOD achieves performance very close to that of the RBox-trained counterpart on remote sensing and other areas, significantly reducing the tedious efforts on labor-intensive annotation for oriented objects. The source codes are available at https://github.com/VisionXLab/whollywood (PyTorch-based) and https://github.com/VisionXLab/whollywood-jittor (Jittor-based).
## QA:
coming soon
