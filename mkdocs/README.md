# arxiv-daily latest papers around arxiv paper daily template
Automated deployment @ 2023-07-26 17:31:20 Asia/Shanghai
> Welcome to contribute! Add your topics and keywords in [`topic.yml`](https://github.com/wanghaisheng/arxiv-paper-daily-template/blob/main/database/topic.yml).
> You can also view historical data through the [storage](https://github.com/wanghaisheng/arxiv-paper-daily-template/blob/main/database/storage).

## brand

### brand
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2023-07-25**|**A Compact DAG for Storing and Searching Maximal Common Subsequences**|Alessio Conte et.al.|[2307.13695v1](http://arxiv.org/abs/2307.13695v1)|null|Maximal Common Subsequences (MCSs) between two strings X and Y are subsequences of both X and Y that are maximal under inclusion. MCSs relax and generalize the well known and widely used concept of Longest Common Subsequences (LCSs), which can be seen as MCSs of maximum length. While the number both LCSs and MCSs can be exponential in the length of the strings, LCSs have been long exploited for string and text analysis, as simple compact representations of all LCSs between two strings, built via dynamic programming or automata, have been known since the '70s. MCSs appear to have a more challenging structure: even listing them efficiently was an open problem open until recently, thus narrowing the complexity difference between the two problems, but the gap remained significant. In this paper we close the complexity gap: we show how to build DAG of polynomial size-in polynomial time-which allows for efficient operations on the set of all MCSs such as enumeration in Constant Amortized Time per solution (CAT), counting, and random access to the i-th element (i.e., rank and select operations). Other than improving known algorithmic results, this work paves the way for new sequence analysis methods based on MCSs.|
|**2023-07-25**|**RED CoMETS: An ensemble classifier for symbolically represented multivariate time series**|Luca A. Bennett et.al.|[2307.13679v1](http://arxiv.org/abs/2307.13679v1)|[link](https://github.com/zy18811/red-comets)|Multivariate time series classification is a rapidly growing research field with practical applications in finance, healthcare, engineering, and more. The complexity of classifying multivariate time series data arises from its high dimensionality, temporal dependencies, and varying lengths. This paper introduces a novel ensemble classifier called RED CoMETS (Random Enhanced Co-eye for Multivariate Time Series), which addresses these challenges. RED CoMETS builds upon the success of Co-eye, an ensemble classifier specifically designed for symbolically represented univariate time series, and extends its capabilities to handle multivariate data. The performance of RED CoMETS is evaluated on benchmark datasets from the UCR archive, where it demonstrates competitive accuracy when compared to state-of-the-art techniques in multivariate settings. Notably, it achieves the highest reported accuracy in the literature for the 'HandMovementDirection' dataset. Moreover, the proposed method significantly reduces computation time compared to Co-eye, making it an efficient and effective choice for multivariate time series classification.|
|**2023-07-25**|**Safety Margins for Reinforcement Learning**|Alexander Grushin et.al.|[2307.13642v1](http://arxiv.org/abs/2307.13642v1)|null|Any autonomous controller will be unsafe in some situations. The ability to quantitatively identify when these unsafe situations are about to occur is crucial for drawing timely human oversight in, e.g., freight transportation applications. In this work, we demonstrate that the true criticality of an agent's situation can be robustly defined as the mean reduction in reward given some number of random actions. Proxy criticality metrics that are computable in real-time (i.e., without actually simulating the effects of random actions) can be compared to the true criticality, and we show how to leverage these proxy metrics to generate safety margins, which directly tie the consequences of potentially incorrect actions to an anticipated loss in overall performance. We evaluate our approach on learned policies from APE-X and A3C within an Atari environment, and demonstrate how safety margins decrease as agents approach failure states. The integration of safety margins into programs for monitoring deployed agents allows for the real-time identification of potentially catastrophic situations.|
|**2023-07-25**|**Developing a Comprehensive Model for Feasibility Analysis of Separated Bike Lanes and Electric Bike Lanes: A Case Study in Shanghai, China**|Lu Ling et.al.|[2307.13628v1](http://arxiv.org/abs/2307.13628v1)|null|Electric bikes (e-bikes), including lightweight e-bikes with pedals and e-bikes in scooter form, are gaining popularity around the world because of their convenience and affordability. At the same time, e-bike-related accidents are also on the rise and many policymakers and practitioners are debating the feasibility of building e-bike lanes in their communities. By collecting e-bikes and bikes data in Shanghai City, the study first recalibrates the capacity of the conventional bike lane based on the traffic movement characteristics of the mixed bikes flow. Then, the study evaluates the traffic safety performance of the mixed bike flow in the conventional bike lane by the observed passing events. Finally, this study proposes a comprehensive model for evaluating the feasibility of building an e-bike lane by integrating the Analytic Hierarchy Process and fuzzy mathematics by considering the three objectives: capacity, safety, and budget constraint. The proposed model, one of the first of its kind, can be used to (i) evaluate the existing road capacity and safety performance improvement of a mixed bike flow with e-bikes and human-powered bikes by analyzing the mixed bike flow arrival rate and passing maneuvers, and (ii) quantify the changes to the road capacity and safety performance if a new e-bike lane is constructed. Numerical experiments are performed to calibrate the proposed model and evaluate its performance using non-motorized vehicles' trajectories in Shanghai, China. The numerical experiment results suggest that the proposed model can be used by policymakers and practitioners to evaluate the feasibility of building e-bike lanes.|
|**2023-07-25**|**RecursiveDet: End-to-End Region-based Recursive Object Detection**|Jing Zhao et.al.|[2307.13619v1](http://arxiv.org/abs/2307.13619v1)|null|End-to-end region-based object detectors like Sparse R-CNN usually have multiple cascade bounding box decoding stages, which refine the current predictions according to their previous results. Model parameters within each stage are independent, evolving a huge cost. In this paper, we find the general setting of decoding stages is actually redundant. By simply sharing parameters and making a recursive decoder, the detector already obtains a significant improvement. The recursive decoder can be further enhanced by positional encoding (PE) of the proposal box, which makes it aware of the exact locations and sizes of input bounding boxes, thus becoming adaptive to proposals from different stages during the recursion. Moreover, we also design centerness-based PE to distinguish the RoI feature element and dynamic convolution kernels at different positions within the bounding box. To validate the effectiveness of the proposed method, we conduct intensive ablations and build the full model on three recent mainstream region-based detectors. The RecusiveDet is able to achieve obvious performance boosts with even fewer model parameters and slightly increased computation cost. Codes are available at https://github.com/bravezzzzzz/RecursiveDet.|
|**2023-07-25**|**Decisive Data using Multi-Modality Optical Sensors for Advanced Vehicular Systems**|Muhammad Ali Farooq et.al.|[2307.13600v1](http://arxiv.org/abs/2307.13600v1)|null|Optical sensors have played a pivotal role in acquiring real world data for critical applications. This data, when integrated with advanced machine learning algorithms provides meaningful information thus enhancing human vision. This paper focuses on various optical technologies for design and development of state-of-the-art out-cabin forward vision systems and in-cabin driver monitoring systems. The focused optical sensors include Longwave Thermal Imaging (LWIR) cameras, Near Infrared (NIR), Neuromorphic/ event cameras, Visible CMOS cameras and Depth cameras. Further the paper discusses different potential applications which can be employed using the unique strengths of each these optical modalities in real time environment.|
|**2023-07-25**|**Multiwavelength observations of PSR J2021+4026 across a mode change reveal a phase shift in its X-ray emission**|M. Razzano et.al.|[2307.13580v1](http://arxiv.org/abs/2307.13580v1)|null|Context. We have investigated the multiwavelength emission of PSR J2021+4026, the only isolated gamma-ray pulsar known to be variable, which in October 2011 underwent a simultaneous change in gamma-ray flux and spin-down rate, followed by a second mode change in February 2018. Multiwavelength monitoring is crucial to understand the physics behind these events and how they may have affected the structure of the magnetosphere. Aims.The monitoring of pulse profile alignment is a powerful diagnostic tool for constraining magnetospheric reconfiguration. We aim to investigate timing or flux changes related to the variability of PSR J2021+4026 via multiwavelength observations, including gamma-ray observations from Fermi-LAT, X-ray observations from XMM-Newton, and a deep optical observation with the Gran Telescopio Canarias.Methods. We performed a detailed comparison of the timing features of the pulsar in gamma and X-rays and searched for any change in phase lag between the phaseogram peaks in these two energy bands. Although previous observations did not detect a counterpart in visible light, we also searched for optical emission that might have increased due to the mode change, making this pulsar detectable in the optical. Results.We have found a change in the gamma-to X-ray pulse profile alignment by 0.21$\pm$0.02 in phase, which indicates that the first mode change affected different regions of the pulsar magnetosphere. No optical counterpart was detected down to g'=26.1 and r'=25.3. Conclusions.We suggest that the observed phase shift could be related to a reconfiguration of the connection between the quadrupole magnetic field near the stellar surface and the dipole field that dominates at larger distances. This is consistent with the picture of X-ray emission coming from the heated polar cap and with the simultaneous flux and frequency derivative change observed during the mode changes.|
|**2023-07-25**|**XDLM: Cross-lingual Diffusion Language Model for Machine Translation**|Linyao Chen et.al.|[2307.13560v1](http://arxiv.org/abs/2307.13560v1)|null|Recently, diffusion models have excelled in image generation tasks and have also been applied to neural language processing (NLP) for controllable text generation. However, the application of diffusion models in a cross-lingual setting is less unexplored. Additionally, while pretraining with diffusion models has been studied within a single language, the potential of cross-lingual pretraining remains understudied. To address these gaps, we propose XDLM, a novel Cross-lingual diffusion model for machine translation, consisting of pretraining and fine-tuning stages. In the pretraining stage, we propose TLDM, a new training objective for mastering the mapping between different languages; in the fine-tuning stage, we build up the translation system based on the pretrained model. We evaluate the result on several machine translation benchmarks and outperformed both diffusion and Transformer baselines.|
|**2023-07-25**|**Network Traffic Classification based on Single Flow Time Series Analysis**|Josef Koumar et.al.|[2307.13434v1](http://arxiv.org/abs/2307.13434v1)|[link](https://github.com/koumajos/classificationbasedonsfts)|Network traffic monitoring using IP flows is used to handle the current challenge of analyzing encrypted network communication. Nevertheless, the packet aggregation into flow records naturally causes information loss; therefore, this paper proposes a novel flow extension for traffic features based on the time series analysis of the Single Flow Time series, i.e., a time series created by the number of bytes in each packet and its timestamp. We propose 69 universal features based on the statistical analysis of data points, time domain analysis, packet distribution within the flow timespan, time series behavior, and frequency domain analysis. We have demonstrated the usability and universality of the proposed feature vector for various network traffic classification tasks using 15 well-known publicly available datasets. Our evaluation shows that the novel feature vector achieves classification performance similar or better than related works on both binary and multiclass classification tasks. In more than half of the evaluated tasks, the classification performance increased by up to 5\%.|
|**2023-07-25**|**Multi-Objective Optimisation of URLLC-Based Metaverse Services**|Xinyu Gao et.al.|[2307.13429v1](http://arxiv.org/abs/2307.13429v1)|null|Metaverse aims for building a fully immersive virtual shared space, where the users are able to engage in various activities. To successfully deploy the service for each user, the Metaverse service provider and network service provider generally localise the user first and then support the communication between the base station (BS) and the user. A reconfigurable intelligent surface (RIS) is capable of creating a reflected link between the BS and the user to enhance line-of-sight. Furthermore, the new key performance indicators (KPIs) in Metaverse, such as its energy-consumption-dependent total service cost and transmission latency, are often overlooked in ultra-reliable low latency communication (URLLC) designs, which have to be carefully considered in next-generation URLLC (xURLLC) regimes. In this paper, our design objective is to jointly optimise the transmit power, the RIS phase shifts, and the decoding error probability to simultaneously minimise the total service cost and transmission latency and approach the Pareto Front (PF). We conceive a twin-stage central controller, which aims for localising the users first and then supports the communication between the BS and users. In the first stage, we localise the Metaverse users, where the stochastic gradient descent (SGD) algorithm is invoked for accurate user localisation. In the second stage, a meta-learning-based position-dependent multi-objective soft actor and critic (MO-SAC) algorithm is proposed to approach the PF between the total service cost and transmission latency and to further optimise the latency-dependent reliability. Our numerical results demonstrate that ...|
|**2023-07-25**|**A signal processing interpretation of noise-reduction convolutional neural networks**|Luis A. Zavala-Mondragón et.al.|[2307.13425v1](http://arxiv.org/abs/2307.13425v1)|null|Encoding-decoding CNNs play a central role in data-driven noise reduction and can be found within numerous deep-learning algorithms. However, the development of these CNN architectures is often done in ad-hoc fashion and theoretical underpinnings for important design choices is generally lacking. Up to this moment there are different existing relevant works that strive to explain the internal operation of these CNNs. Still, these ideas are either scattered and/or may require significant expertise to be accessible for a bigger audience. In order to open up this exciting field, this article builds intuition on the theory of deep convolutional framelets and explains diverse ED CNN architectures in a unified theoretical framework. By connecting basic principles from signal processing to the field of deep learning, this self-contained material offers significant guidance for designing robust and efficient novel CNN architectures.|
|**2023-07-25**|**Co-Design of Out-of-Distribution Detectors for Autonomous Emergency Braking Systems**|Michael Yuhas et.al.|[2307.13419v1](http://arxiv.org/abs/2307.13419v1)|null|Learning enabled components (LECs), while critical for decision making in autonomous vehicles (AVs), are likely to make incorrect decisions when presented with samples outside of their training distributions. Out-of-distribution (OOD) detectors have been proposed to detect such samples, thereby acting as a safety monitor, however, both OOD detectors and LECs require heavy utilization of embedded hardware typically found in AVs. For both components, there is a tradeoff between non-functional and functional performance, and both impact a vehicle's safety. For instance, giving an OOD detector a longer response time can increase its accuracy at the expense of the LEC. We consider an LEC with binary output like an autonomous emergency braking system (AEBS) and use risk, the combination of severity and occurrence of a failure, to model the effect of both components' design parameters on each other's functional and non-functional performance, as well as their impact on system safety. We formulate a co-design methodology that uses this risk model to find the design parameters for an OOD detector and LEC that decrease risk below that of the baseline system and demonstrate it on a vision based AEBS. Using our methodology, we achieve a 42.3% risk reduction while maintaining equivalent resource utilization.|
|**2023-07-25**|**Hybrid Goldstone Modes from the Double Copy Bootstrap**|Yang Li et.al.|[2307.13418v1](http://arxiv.org/abs/2307.13418v1)|null|We perform a systematic classification of scalar field theories whose amplitudes admit a double copy formulation and identify two building blocks at 4-point and 13 at 5-point. Using the 4-point blocks as bootstrap seeds, this naturally leads to a single copy theory that is a gauged NLSM. Moreover, its double copy includes a novel theory that can be written in terms of Lovelock invariants of an induced metric, and includes Dirac-Born-Infeld and the special Galileon in specific limits. The amplitudes of these Goldstone modes have two distinct soft behaviour regimes, corresponding to a hybrid of non-linear symmetries.|
|**2023-07-25**|**Probe thermometry with continuous measurements**|Julia Boeyens et.al.|[2307.13407v1](http://arxiv.org/abs/2307.13407v1)|null|Temperature estimation plays a vital role across natural sciences. A standard approach is provided by probe thermometry, where a probe is brought into contact with the sample and examined after a certain amount of time has passed. In many situations however, continuously monitoring the probe may be preferred. Here, we consider a minimal model, where the probe is provided by a two-level system coupled to a thermal reservoir. Monitoring thermally activated transitions enables real-time estimation of temperature with increasing accuracy over time. Within this framework we comprehensively investigate thermometry in both bosonic and fermionic environments employing a Bayesian approach. Furthermore, we explore adaptive strategies and find a significant improvement on the precision. Additionally, we examine the impact of noise and find that adaptive strategies may suffer more than non-adaptive ones for short observation times. While our main focus is on thermometry, our results are easily extended to the estimation of other environmental parameters, such as chemical potentials and transition rates.|
|**2023-07-25**|**Towards Bridging the Digital Language Divide**|Gábor Bella et.al.|[2307.13405v1](http://arxiv.org/abs/2307.13405v1)|null|It is a well-known fact that current AI-based language technology -- language models, machine translation systems, multilingual dictionaries and corpora -- focuses on the world's 2-3% most widely spoken languages. Recent research efforts have attempted to expand the coverage of AI technology to `under-resourced languages.' The goal of our paper is to bring attention to a phenomenon that we call linguistic bias: multilingual language processing systems often exhibit a hardwired, yet usually involuntary and hidden representational preference towards certain languages. Linguistic bias is manifested in uneven per-language performance even in the case of similar test conditions. We show that biased technology is often the result of research and development methodologies that do not do justice to the complexity of the languages being represented, and that can even become ethically problematic as they disregard valuable aspects of diversity as well as the needs of the language communities themselves. As our attempt at building diversity-aware language resources, we present a new initiative that aims at reducing linguistic bias through both technological design and methodology, based on an eye-level collaboration with local communities.|
|**2023-07-25**|**Predicting Code Coverage without Execution**|Michele Tufano et.al.|[2307.13383v1](http://arxiv.org/abs/2307.13383v1)|[link](https://github.com/microsoft/coverage-eval)|Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branches, are executed during testing. Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the instrumentation. Furthermore, computing coverage of any snippet of code requires the whole program context. Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the source code context, and the task of code coverage prediction can be a novel benchmark for judging the ability of models to understand code. We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs). We formalize this task to evaluate the capability of LLMs in understanding code execution by determining which lines of a method are executed by a given test case and inputs. We curate and release a dataset we call COVERAGEEVAL by executing tests and code from the HumanEval dataset and collecting code coverage information. We report the performance of four state-of-the-art LLMs used for code-related tasks, including OpenAI's GPT-4 and GPT-3.5-Turbo, Google's BARD, and Anthropic's Claude, on the Code Coverage Prediction task. Finally, we argue that code coverage as a metric and pre-training data source are valuable for overall LLM performance on software engineering tasks.|
|**2023-07-25**|**Submodular Reinforcement Learning**|Manish Prajapat et.al.|[2307.13372v1](http://arxiv.org/abs/2307.13372v1)|[link](https://github.com/manish-pra/non-additive-rl)|In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are $\textit{independent}$ of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose $\textit{submodular RL}$ (SubRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SubPO, a simple policy gradient-based algorithm for SubRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SubPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing SubRL instances even in large state- and action- spaces. We showcase the versatility of our approach by applying SubPO to several applications, such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.|
|**2023-07-25**|**Computational Guarantees for Doubly Entropic Wasserstein Barycenters via Damped Sinkhorn Iterations**|Lénaïc Chizat et.al.|[2307.13370v1](http://arxiv.org/abs/2307.13370v1)|null|We study the computation of doubly regularized Wasserstein barycenters, a recently introduced family of entropic barycenters governed by inner and outer regularization strengths. Previous research has demonstrated that various regularization parameter choices unify several notions of entropy-penalized barycenters while also revealing new ones, including a special case of debiased barycenters. In this paper, we propose and analyze an algorithm for computing doubly regularized Wasserstein barycenters. Our procedure builds on damped Sinkhorn iterations followed by exact maximization/minimization steps and guarantees convergence for any choice of regularization parameters. An inexact variant of our algorithm, implementable using approximate Monte Carlo sampling, offers the first non-asymptotic convergence guarantees for approximating Wasserstein barycenters between discrete point clouds in the free-support/grid-free setting.|
|**2023-07-25**|**Continuous sensing and parameter estimation with the boundary time-crystal**|Albert Cabot et.al.|[2307.13277v1](http://arxiv.org/abs/2307.13277v1)|null|A boundary time-crystal is a quantum many-body system whose dynamics is governed by the competition between coherent driving and collective dissipation. It is composed of N two-level systems and features a transition between a stationary phase and an oscillatory one. The fact that the system is open allows to continuously monitor its quantum trajectories and to analyze their dependence on parameter changes. This enables the realization of a sensing device whose performance we investigate as a function of the monitoring time T and of the system size N. We find that the best achievable sensitivity is proportional to $\sqrt{T}N$, i.e., it follows the standard quantum limit in time and Heisenberg scaling in the particle number. This theoretical scaling can be achieved in the oscillatory time-crystal phase and it is rooted in emergent quantum correlations. The main challenge is, however, to tap this capability in a measurement protocol that is experimentally feasible. We demonstrate that the standard quantum limit can be surpassed by cascading two time-crystals, where the quantum trajectories of one time-crystal are used as input for the other one.|
|**2023-07-25**|**Ellipsoidal superpotentials and stationary descendants**|Grigory Mikhalkin et.al.|[2307.13252v1](http://arxiv.org/abs/2307.13252v1)|null|We compute stationary gravitational descendants in symplectic ellipsoids of any dimension, and use these to derive a number of new recursive formula for punctured curve counts in symplectic manifolds with ellipsoidal ends. Along the way we develop a framework in which punctured curve counts can be explicitly computed using the standard complex structure on affine space. Finally, we initiate the study of "infinitesimal symplectic cobordisms", which serve as elementary building blocks for symplectic cobordisms between ellipsoids.|
|**2023-07-25**|**BIM-to-BRICK: Using graph modeling for IoT/BMS and spatial semantic data interoperability within digital data models of buildings**|Filippo Vittori et.al.|[2307.13197v1](http://arxiv.org/abs/2307.13197v1)|null|The holistic management of a building requires data from heterogeneous sources such as building management systems (BMS), Internet-of-Things (IoT) sensor networks, and building information models. Data interoperability is a key component to eliminate silos of information, and using semantic web technologies like the BRICK schema, an effort to standardize semantic descriptions of the physical, logical, and virtual assets in buildings and the relationships between them, is a suitable approach. However, current data integration processes can involve significant manual interventions. This paper presents a methodology to automatically collect, assemble, and integrate information from a building information model to a knowledge graph. The resulting application, called BIM-to-BRICK, is run on the SDE4 building located in Singapore. BIM-to-BRICK generated a bidirectional link between a BIM model of 932 instances and experimental data collected for 17 subjects into 458 BRICK objects and 1219 relationships in 17 seconds. The automation of this approach can be compared to traditional manual mapping of data types. This scientific innovation incentivizes the convergence of disparate data types and structures in built-environment applications.|
|**2023-07-24**|**Evaluating the reliability of automatically generated pedestrian and bicycle crash surrogates**|Agnimitra Sengupta et.al.|[2307.13178v1](http://arxiv.org/abs/2307.13178v1)|null|Vulnerable road users (VRUs), such as pedestrians and bicyclists, are at a higher risk of being involved in crashes with motor vehicles, and crashes involving VRUs also are more likely to result in severe injuries or fatalities. Signalized intersections are a major safety concern for VRUs due to their complex and dynamic nature, highlighting the need to understand how these road users interact with motor vehicles and deploy evidence-based countermeasures to improve safety performance. Crashes involving VRUs are relatively infrequent, making it difficult to understand the underlying contributing factors. An alternative is to identify and use conflicts between VRUs and motorized vehicles as a surrogate for safety performance. Automatically detecting these conflicts using a video-based systems is a crucial step in developing smart infrastructure to enhance VRU safety. The Pennsylvania Department of Transportation conducted a study using video-based event monitoring system to assess VRU and motor vehicle interactions at fifteen signalized intersections across Pennsylvania to improve VRU safety performance. This research builds on that study to assess the reliability of automatically generated surrogates in predicting confirmed conflicts using advanced data-driven models. The surrogate data used for analysis include automatically collectable variables such as vehicular and VRU speeds, movements, post-encroachment time, in addition to manually collected variables like signal states, lighting, and weather conditions. The findings highlight the varying importance of specific surrogates in predicting true conflicts, some being more informative than others. The findings can assist transportation agencies to collect the right types of data to help prioritize infrastructure investments, such as bike lanes and crosswalks, and evaluate their effectiveness.|
|**2023-07-24**|**A Differentially Private Weighted Empirical Risk Minimization Procedure and its Application to Outcome Weighted Learning**|Spencer Giddens et.al.|[2307.13127v1](http://arxiv.org/abs/2307.13127v1)|null|It is commonplace to use data containing personal information to build predictive models in the framework of empirical risk minimization (ERM). While these models can be highly accurate in prediction, results obtained from these models with the use of sensitive data may be susceptible to privacy attacks. Differential privacy (DP) is an appealing framework for addressing such data privacy issues by providing mathematically provable bounds on the privacy loss incurred when releasing information from sensitive data. Previous work has primarily concentrated on applying DP to unweighted ERM. We consider an important generalization to weighted ERM (wERM). In wERM, each individual's contribution to the objective function can be assigned varying weights. In this context, we propose the first differentially private wERM algorithm, backed by a rigorous theoretical proof of its DP guarantees under mild regularity conditions. Extending the existing DP-ERM procedures to wERM paves a path to deriving privacy-preserving learning methods for individualized treatment rules, including the popular outcome weighted learning (OWL). We evaluate the performance of the DP-wERM application to OWL in a simulation study and in a real clinical trial of melatonin for sleep health. All empirical results demonstrate the viability of training OWL models via wERM with DP guarantees while maintaining sufficiently useful model performance. Therefore, we recommend practitioners consider implementing the proposed privacy-preserving OWL procedure in real-world scenarios involving sensitive data.|
|**2023-07-24**|**Conformal prediction for frequency-severity modeling**|Helton Graziadei et.al.|[2307.13124v1](http://arxiv.org/abs/2307.13124v1)|[link](https://github.com/heltongraziadei/conformal-fs)|We present a nonparametric model-agnostic framework for building prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The effectiveness of the framework is showcased with simulated and real datasets. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction procedure, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set and to enable the production of prediction intervals with adaptive width.|
|**2023-07-24**|**Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark**|Sai Kumar Reddy Manne et.al.|[2307.13110v1](http://arxiv.org/abs/2307.13110v1)|[link](https://github.com/ostadabbas/infant-respiration-estimation)|Respiration is a critical vital sign for infants, and continuous respiratory monitoring is particularly important for newborns. However, neonates are sensitive and contact-based sensors present challenges in comfort, hygiene, and skin health, especially for preterm babies. As a step toward fully automatic, continuous, and contactless respiratory monitoring, we develop a deep-learning method for estimating respiratory rate and waveform from plain video footage in natural settings. Our automated infant respiration flow-based network (AIRFlowNet) combines video-extracted optical flow input and spatiotemporal convolutional processing tuned to the infant domain. We support our model with the first public annotated infant respiration dataset with 125 videos (AIR-125), drawn from eight infant subjects, set varied pose, lighting, and camera conditions. We include manual respiration annotations and optimize AIRFlowNet training on them using a novel spectral bandpass loss function. When trained and tested on the AIR-125 infant data, our method significantly outperforms other state-of-the-art methods in respiratory rate estimation, achieving a mean absolute error of $\sim$2.9 breaths per minute, compared to $\sim$4.7--6.2 for other public models designed for adult subjects and more uniform environments.|
|**2023-07-24**|**Label Noise: Correcting a Correction**|William Toner et.al.|[2307.13100v1](http://arxiv.org/abs/2307.13100v1)|null|Training neural network classifiers on datasets with label noise poses a risk of overfitting them to the noisy labels. To address this issue, researchers have explored alternative loss functions that aim to be more robust. However, many of these alternatives are heuristic in nature and still vulnerable to overfitting or underfitting. In this work, we propose a more direct approach to tackling overfitting caused by label noise. We observe that the presence of label noise implies a lower bound on the noisy generalised risk. Building upon this observation, we propose imposing a lower bound on the empirical risk during training to mitigate overfitting. Our main contribution is providing theoretical results that yield explicit, easily computable bounds on the minimum achievable noisy risk for different loss functions. We empirically demonstrate that using these bounds significantly enhances robustness in various settings, with virtually no additional computational cost.|
|**2023-07-24**|**Fairness Under Demographic Scarce Regime**|Patrik Joslin Kenfack et.al.|[2307.13081v1](http://arxiv.org/abs/2307.13081v1)|[link](https://github.com/patrikken/fair-dsr)|Most existing works on fairness assume the model has full access to demographic information. However, there exist scenarios where demographic information is partially available because a record was not maintained throughout data collection or due to privacy reasons. This setting is known as demographic scarce regime. Prior research have shown that training an attribute classifier to replace the missing sensitive attributes (proxy) can still improve fairness. However, the use of proxy-sensitive attributes worsens fairness-accuracy trade-offs compared to true sensitive attributes. To address this limitation, we propose a framework to build attribute classifiers that achieve better fairness-accuracy trade-offs. Our method introduces uncertainty awareness in the attribute classifier and enforces fairness on samples with demographic information inferred with the lowest uncertainty. We show empirically that enforcing fairness constraints on samples with uncertain sensitive attributes is detrimental to fairness and accuracy. Our experiments on two datasets showed that the proposed framework yields models with significantly better fairness-accuracy trade-offs compared to classic attribute classifiers. Surprisingly, our framework outperforms models trained with constraints on the true sensitive attributes.|
|**2023-07-24**|**Orthogonal-view Microscope for the Biomechanics Investigations of Aquatic Organisms**|Brian T. Le et.al.|[2307.13079v1](http://arxiv.org/abs/2307.13079v1)|null|Microscopes are essential for biomechanics and hydrodynamical investigation of small aquatic organisms. We report a DIY microscope (GLUBscope) that enables the visualization of organisms from two orthogonal imaging planes (top and side views). Compared to conventional imaging systems, this approach provides a comprehensive visualization strategy of organisms, which could have complex shapes and morphologies. The microscope was constructed by combining custom 3D-printed parts and off-the-shelf components. The system is designed for modularity and reconfigurability. Open-source design files and build instructions are provided in this report. Additionally, proof of use experiments, particularly with Hydra and other organisms that combine the GLUBscope with an analysis pipeline, were demonstrated. Beyond the applications demonstrated, the system can be used or modified for various imaging applications.|
|**2023-07-24**|**The IceCube-Gen2 Collaboration -- Contributions to the 38th International Cosmic Ray Conference (ICRC2023)**|IceCube-Gen2 et.al.|[2307.13048v1](http://arxiv.org/abs/2307.13048v1)|null|IceCube-Gen2 is a planned next-generation neutrino observatory at the South Pole that builds upon the successful design of IceCube. Integrating two complementary detection technologies for neutrinos, optical and radio Cherenkov emission, in combination with a surface array for cosmic ray air shower detection, IceCube-Gen2 will cover a broad neutrino energy range from MeV to EeV. This index of contributions to the 38th International Cosmic Ray Conference in Nagoya, Japan (July 26 - August 3, 2023) describes research and development efforts for IceCube-Gen2. Included are summaries of the design, status, and sensitivity of the IceCube-Gen2 optical, surface, and radio components; performance studies of next-generation optical sensors detecting optical Cherenkov radiation from cosmic ray and neutrino events; reconstruction techniques of radio and optical events in terms of energy, direction, and neutrino flavor; and sensitivity studies of astrophysical neutrino flavors, diffuse neutrino fluxes, and cosmic ray anisotropies. Contributions related to IceCube and the scheduled IceCube Upgrade are available in a separate collection.|
|**2023-07-24**|**Multipoint fishnet Feynman diagrams: sequential splitting**|Francesco Aprile et.al.|[2307.12984v1](http://arxiv.org/abs/2307.12984v1)|null|We study fishnet Feynman diagrams defined by a certain triangulation of a planar n-gon, with massless scalars propagating along and across the cuts. Our solution theory uses the technique of Separation of Variables, in combination with the theory of symmetric polynomials and Mellin space. The n-point split-ladders are solved by a recursion where all building blocks are made fully explicit. In particular, we find an elegant formula for the coefficient functions of the light-cone leading logs. When the diagram grows into a fishnet, we obtain new results exploiting a Cauchy identity decomposition of the measure over separated variables. This leads to an elementary proof of the Basso-Dixon formula at 4-points, while at n-points it provides a natural OPE-like stratification of the diagram. Finally, we propose an independent approach based on ``stampede" combinatorics to study the light-cone behaviour of the diagrams as the partition function of a certain vertex model.|
