
### all search terms
|Publish Date|Title|Authors|PDF|Code|Abstract|
| :---: | :---: | :---: | :---: | :---: | :---: |
|**2024-07-05**|**Thermal and mechanical study of a parametrised cryostat model for optical characterisation of upcoming CMB experiments**|Thomas J. L. J. Gascard et.al.|[2407.04613v1](http://arxiv.org/abs/2407.04613v1)|null|Current and future experiments observing the cosmic microwave background require a detailed understanding of optical performance at cryogenic temperatures. Pre-deployment analysis of optics can be performed in custom-engineered cryogenic test beds, such as Mod-Cam, a first light camera for the CCAT project. This work presents studies of the mechanical and thermal performance of CryoSim, a model of a generic cylindrical 4-K cryostat cooled with a commercial pulse tube cryocooler that can be used to characterise optical components and full reimaging optical systems. CryoSim is extensively parametrised, allowing the joint analysis and optimisation of mechanical and thermal performance via finite element methods. Results from this model are validated against measured cooldown data of the Mod-Cam cryostat. Due to the extensive parametrisation of the model, significant modifications of the cryostat geometry may be implemented to be representative of any system the scientific community may desire, and validation of thermal and mechanical performance can be carried out rapidly.|
|**2024-07-05**|**Real Time Emotion Analysis Using Deep Learning for Education, Entertainment, and Beyond**|Abhilash Khuntia et.al.|[2407.04560v1](http://arxiv.org/abs/2407.04560v1)|null|The significance of emotion detection is increasing in education, entertainment, and various other domains. We are developing a system that can identify and transform facial expressions into emojis to provide immediate feedback.The project consists of two components. Initially, we will employ sophisticated image processing techniques and neural networks to construct a deep learning model capable of precisely categorising facial expressions. Next, we will develop a basic application that records live video using the camera on your device. The app will utilise a sophisticated model to promptly analyse facial expressions and promptly exhibit corresponding emojis.Our objective is to develop a dynamic tool that integrates deep learning and real-time video processing for the purposes of online education, virtual events, gaming, and enhancing user experience. This tool enhances interactions and introduces novel emotional intelligence technologies.|
|**2024-07-05**|**A Novel Portable and Wearable Broadband Near-Infrared Spectroscopy Device for In-Vivo Oxygenation and Metabolism Measurements**|Musa Talati et.al.|[2407.04515v1](http://arxiv.org/abs/2407.04515v1)|null|Broadband NIRS (bNIRS) is an extension of fNIRS that provides the same assessment of oxygenation biomarkers along with a valuable marker for oxygen metabolism at a cellular level, the oxidation state of cytochrome-c-oxidase (oxCCO). bNIRS implements many (100s) NIR wavelengths in the full NIR spectrum to address this and provide insight to tissue energetics. To supply these many wavelengths of light, broadband sources are required, and spectrometers are employed to distinguish power per wavelength. Current multi-channel bNIRS instruments are bulky and only semi-portable due to technological limitations. We propose a design for a bNIRS device that has been miniaturized to allow for portable use. This design leverages the innovations in photonic devices that have created a new line of microspectrometers and broadband NIR high-power LEDs; the Hamamatsu SMD-type spectrometer C14384MA and the Ushio SMBBIR45-1100 LED. This first-of-itskind device, referred to as microCYRIL (after its two predecessors CYRIL and miniCYRIL), has been developed for oxygenation and metabolism measurements with dual channel operation. To verify functionality, concentration changes in oxygenated (HbO2) and deoxygenated (HHb) haemoglobin and oxCCO were successfully tracked during a cuff-induced venous and arterial occlusion.|
|**2024-07-05**|**Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos**|Leonhard Sommer et.al.|[2407.04384v1](http://arxiv.org/abs/2407.04384v1)|[link](https://github.com/genintel/uns-obj-pose3d)|Category-level 3D pose estimation is a fundamentally important problem in computer vision and robotics, e.g. for embodied agents or to train 3D generative models. However, so far methods that estimate the category-level object pose require either large amounts of human annotations, CAD models or input from RGB-D sensors. In contrast, we tackle the problem of learning to estimate the category-level 3D pose only from casually taken object-centric videos without human supervision. We propose a two-step pipeline: First, we introduce a multi-view alignment procedure that determines canonical camera poses across videos with a novel and robust cyclic distance formulation for geometric and appearance matching using reconstructed coarse meshes and DINOv2 features. In a second step, the canonical poses and reconstructed meshes enable us to train a model for 3D pose estimation from a single image. In particular, our model learns to estimate dense correspondences between images and a prototypical 3D template by predicting, for each pixel in a 2D image, a feature vector of the corresponding vertex in the template mesh. We demonstrate that our method outperforms all baselines at the unsupervised alignment of object-centric videos by a large margin and provides faithful and robust predictions in-the-wild. Our code and data is available at https://github.com/GenIntel/uns-obj-pose3d.|
|**2024-07-05**|**TF-SASM: Training-free Spatial-aware Sparse Memory for Multi-object Tracking**|Thuc Nguyen-Quang et.al.|[2407.04327v1](http://arxiv.org/abs/2407.04327v1)|null|Multi-object tracking (MOT) in computer vision remains a significant challenge, requiring precise localization and continuous tracking of multiple objects in video sequences. This task is crucial for various applications, including action recognition and behavior analysis. Key challenges include occlusion, reidentification, tracking fast-moving objects, and handling camera motion artifacts. Past research has explored tracking-by-detection methods and end-to-end models, with recent attention on tracking-by-attention approaches leveraging transformer architectures. The emergence of data sets that emphasize robust reidentification, such as DanceTrack, has highlighted the need for effective solutions. While memory-based approaches have shown promise, they often suffer from high computational complexity and memory usage. We propose a novel sparse memory approach that selectively stores critical features based on object motion and overlapping awareness, aiming to enhance efficiency while minimizing redundancy. Building upon the MOTRv2 model, a hybrid of tracking-by-attention and tracking-by-detection, we introduce a training-free memory designed to bolster reidentification capabilities and preserve the model's flexibility. Our memory approach achieves significant improvements over MOTRv2 in the DanceTrack test set, demonstrating a gain of 1.1\% in HOTA metrics and 2.1\% in IDF1 score.|
|**2024-07-05**|**Research, Applications and Prospects of Event-Based Pedestrian Detection: A Survey**|Han Wang et.al.|[2407.04277v1](http://arxiv.org/abs/2407.04277v1)|null|Event-based cameras, inspired by the biological retina, have evolved into cutting-edge sensors distinguished by their minimal power requirements, negligible latency, superior temporal resolution, and expansive dynamic range. At present, cameras used for pedestrian detection are mainly frame-based imaging sensors, which have suffered from lethargic response times and hefty data redundancy. In contrast, event-based cameras address these limitations by eschewing extraneous data transmissions and obviating motion blur in high-speed imaging scenarios. On pedestrian detection via event-based cameras, this paper offers an exhaustive review of research and applications particularly in the autonomous driving context. Through methodically scrutinizing relevant literature, the paper outlines the foundational principles, developmental trajectory, and the comparative merits and demerits of eventbased detection relative to traditional frame-based methodologies. This review conducts thorough analyses of various event stream inputs and their corresponding network models to evaluate their applicability across diverse operational environments. It also delves into pivotal elements such as crucial datasets and data acquisition techniques essential for advancing this technology, as well as advanced algorithms for processing event stream data. Culminating with a synthesis of the extant landscape, the review accentuates the unique advantages and persistent challenges inherent in event-based pedestrian detection, offering a prognostic view on potential future developments in this fast-progressing field.|
|**2024-07-05**|**Time-resolved phase-lock pressure-sensitive paint measurement of trailing edge noise dynamics**|M. Imai et.al.|[2407.04233v1](http://arxiv.org/abs/2407.04233v1)|null|Pressure-sensitive paint (PSP) was applied to the surface of a NACA0012 airfoil to investigate pressure fluctuations associated with trailing edge (TE) noise under low-velocity flow conditions. The primary focus is to assess the feasibility of employing laser pulses exposed at the airfoil surface to mitigate TE noise. However, the weak pressure fluctuations accompanying TE noise pose a challenge, as they are overshadowed by image sensor noise in high-speed cameras capturing PSP emission changes. A novel time-resolved phase-locking technique was introduced to address this issue, utilizing the signal from a semiconductor pressure transducer at the trailing edge as a phase-lock trigger source. By repetitively conducting phase-locked measurements (1150 times), time series ensemble-averaged data based on PSP emission images were obtained, enabling the capture of these subtle pressure fluctuations. Quantitatively, fluctuations with a dominant frequency of 679 Hz and an amplitude of 50 Pa are resolved within an accuracy of about 15 Pa, achieved at a recording rate of 19.2 kHz. Both the suppression and subsequent redevelopment of the pressure field with the TE noise offer valuable insights into the dynamics of TE noise and open avenues for targeted noise reduction strategies in aerodynamic applications.|
|**2024-07-04**|**Solutions to Deepfakes: Can Camera Hardware, Cryptography, and Deep Learning Verify Real Images?**|Alexander Vilesov et.al.|[2407.04169v1](http://arxiv.org/abs/2407.04169v1)|null|The exponential progress in generative AI poses serious implications for the credibility of all real images and videos. There will exist a point in the future where 1) digital content produced by generative AI will be indistinguishable from those created by cameras, 2) high-quality generative algorithms will be accessible to anyone, and 3) the ratio of all synthetic to real images will be large. It is imperative to establish methods that can separate real data from synthetic data with high confidence. We define real images as those that were produced by the camera hardware, capturing a real-world scene. Any synthetic generation of an image or alteration of a real image through generative AI or computer graphics techniques is labeled as a synthetic image. To this end, this document aims to: present known strategies in detection and cryptography that can be employed to verify which images are real, weight the strengths and weaknesses of these strategies, and suggest additional improvements to alleviate shortcomings.|
|**2024-07-04**|**Occupancy as Set of Points**|Yiang Shi et.al.|[2407.04049v1](http://arxiv.org/abs/2407.04049v1)|[link](https://github.com/hustvl/osp)|In this paper, we explore a novel point representation for 3D occupancy prediction from multi-view images, which is named Occupancy as Set of Points. Existing camera-based methods tend to exploit dense volume-based representation to predict the occupancy of the whole scene, making it hard to focus on the special areas or areas out of the perception range. In comparison, we present the Points of Interest (PoIs) to represent the scene and propose OSP, a novel framework for point-based 3D occupancy prediction. Owing to the inherent flexibility of the point-based representation, OSP achieves strong performance compared with existing methods and excels in terms of training and inference adaptability. It extends beyond traditional perception boundaries and can be seamlessly integrated with volume-based methods to significantly enhance their effectiveness. Experiments on the Occ3D nuScenes occupancy benchmark show that OSP has strong performance and flexibility. Code and models are available at \url{https://github.com/hustvl/osp}.|
|**2024-07-04**|**Exploring Diachronic and Diatopic Changes in Dialect Continua: Tasks, Datasets and Challenges**|Melis Çelikkol et.al.|[2407.04010v1](http://arxiv.org/abs/2407.04010v1)|null|Everlasting contact between language communities leads to constant changes in languages over time, and gives rise to language varieties and dialects. However, the communities speaking non-standard language are often overlooked by non-inclusive NLP technologies. Recently, there has been a surge of interest in studying diatopic and diachronic changes in dialect NLP, but there is currently no research exploring the intersection of both. Our work aims to fill this gap by systematically reviewing diachronic and diatopic papers from a unified perspective. In this work, we critically assess nine tasks and datasets across five dialects from three language families (Slavic, Romance, and Germanic) in both spoken and written modalities. The tasks covered are diverse, including corpus construction, dialect distance estimation, and dialect geolocation prediction, among others. Moreover, we outline five open challenges regarding changes in dialect use over time, the reliability of dialect datasets, the importance of speaker characteristics, limited coverage of dialects, and ethical considerations in data collection. We hope that our work sheds light on future research towards inclusive computational methods and datasets for language varieties and dialects.|
|**2024-07-04**|**CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images**|Junghe Lee et.al.|[2407.03923v1](http://arxiv.org/abs/2407.03923v1)|null|Neural radiance fields (NeRFs) have received significant attention due to their high-quality novel view rendering ability, prompting research to address various real-world cases. One critical challenge is the camera motion blur caused by camera movement during exposure time, which prevents accurate 3D scene reconstruction. In this study, we propose continuous rigid motion-aware gaussian splatting (CRiM-GS) to reconstruct accurate 3D scene from blurry images with real-time rendering speed. Considering the actual camera motion blurring process, which consists of complex motion patterns, we predict the continuous movement of the camera based on neural ordinary differential equations (ODEs). Specifically, we leverage rigid body transformations to model the camera motion with proper regularization, preserving the shape and size of the object. Furthermore, we introduce a continuous deformable 3D transformation in the \textit{SE(3)} field to adapt the rigid body transformation to real-world problems by ensuring a higher degree of freedom. By revisiting fundamental camera theory and employing advanced neural network training techniques, we achieve accurate modeling of continuous camera trajectories. We conduct extensive experiments, demonstrating state-of-the-art performance both quantitatively and qualitatively on benchmark datasets.|
|**2024-07-04**|**Markerless Multi-view 3D Human Pose Estimation: a survey**|Ana Filipa Rodrigues Nogueira et.al.|[2407.03817v1](http://arxiv.org/abs/2407.03817v1)|null|3D human pose estimation aims to reconstruct the human skeleton of all the individuals in a scene by detecting several body joints. The creation of accurate and efficient methods is required for several real-world applications including animation, human-robot interaction, surveillance systems or sports, among many others. However, several obstacles such as occlusions, random camera perspectives, or the scarcity of 3D labelled data, have been hampering the models' performance and limiting their deployment in real-world scenarios. The higher availability of cameras has led researchers to explore multi-view solutions due to the advantage of being able to exploit different perspectives to reconstruct the pose.   Thus, the goal of this survey is to present an overview of the methodologies used to estimate the 3D pose in multi-view settings, understand what were the strategies found to address the various challenges and also, identify their limitations. Based on the reviewed articles, it was possible to find that no method is yet capable of solving all the challenges associated with the reconstruction of the 3D pose. Due to the existing trade-off between complexity and performance, the best method depends on the application scenario. Therefore, further research is still required to develop an approach capable of quickly inferring a highly accurate 3D pose with bearable computation cost. To this goal, techniques such as active learning, methods that learn with a low level of supervision, the incorporation of temporal consistency, view selection, estimation of depth information and multi-modal approaches might be interesting strategies to keep in mind when developing a new methodology to solve this task.|
|**2024-07-04**|**Preliminary results of the Single Event Effect testing for the ULTRASAT sensors**|Vlad Dumitru Berlea et.al.|[2407.03789v1](http://arxiv.org/abs/2407.03789v1)|null|ULTRASAT (ULtra-violet TRansient Astronomy SATellite) is a wide-angle space telescope that will perform a deep time-resolved all-sky survey in the near-ultraviolet (NUV) spectrum. The science objectives are the detection of counterparts to short-lived transient astronomical events such as gravitational wave sources and supernovae. The mission is led by the Weizmann Institute of Science and is planned for launch in 2026 in collaboration with the Israeli Space Agency and NASA. DESY will provide the UV camera, composed by the detector assembly located in the telescope focal plane and the remote electronics unit. The camera is composed out of four back-metallized CMOS Image Sensors (CIS) manufactured in the 4T, dual gain Tower process. As part of the radiation qualification of the camera, Single Event Effect (SEE) testing has been performed by irradiating the sensor with heavy ions at the RADEF, Jyvaskyla facility. Preliminary results of both Single Event Upset (SEU) and Single Event Latch-up (SEL) occurrence rate in the sensor are presented. Additionally, an in-orbit SEE rate simulation has been performed in order to gain preliminary knowledge about the expected effect of SEE on the mission.|
|**2024-07-04**|**SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors**|Yijia Guo et.al.|[2407.03771v1](http://arxiv.org/abs/2407.03771v1)|null|3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance in 3D scene reconstruction. However, 3DGS heavily relies on the sharp images. Fulfilling this requirement can be challenging in real-world scenarios especially when the camera moves fast, which severely limits the application of 3DGS. To address these challenges, we proposed Spike Gausian Splatting (SpikeGS), the first framework that integrates the spike streams into 3DGS pipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With accumulation rasterization, interval supervision, and a specially designed pipeline, SpikeGS extracts detailed geometry and texture from high temporal resolution but texture lacking spike stream, reconstructs 3D scenes captured in 1 second. Extensive experiments on multiple synthetic and real-world datasets demonstrate the superiority of SpikeGS compared with existing spike-based and deblur 3D scene reconstruction methods. Codes and data will be released soon.|
|**2024-07-04**|**A Computer Vision Approach to Estimate the Localized Sea State**|Aleksandar Vorkapic et.al.|[2407.03755v1](http://arxiv.org/abs/2407.03755v1)|null|This research presents a novel application of computer vision (CV) and deep learning methods for real-time sea state recognition aiming to contribute to improving the operational safety and energy efficiency of seagoing vessels, key factors in meeting the International Maritime Organization's carbon reduction targets. In particular, our work focuses on utilizing sea images in operational envelope captured by a single stationary camera mounted on the ship bridge, which are used to train deep learning algorithms for automatic sea state estimation based on the Beaufort scale. To recognize the sea state, we used 4 state-of-the-art neural networks with different characteristics that proved useful in various computer vision tasks: Resnet-101, NASNet, MobileNet_v2 and Transformer Vit-b32. Furthermore, we have defined a unique large-scale dataset, collected over a broad range of sea conditions from an ocean-going vessel prepared for machine learning. We used transfer learning approach to fine-tune the models on our dataset. The obtained results suggest promising potential for this approach to complement traditional methods, particularly where in-situ measurements are unfeasible or interpolated weather buoy data is insufficiently accurate. This study sets the groundwork for further development of machine learning-based sea state classification models to address recognized gaps in maritime research and enable safer and more efficient maritime operations.|
|**2024-07-04**|**Bayesian inference of physics-based models of acoustically-forced laminar premixed conical flames**|Alessandro Giannotta et.al.|[2407.03701v1](http://arxiv.org/abs/2407.03701v1)|null|We perform twenty experiments on an acoustically-forced laminar premixed Bunsen flame and assimilate high-speed footage of the natural emission into a physics-based model containing seven parameters. The experimental rig is a ducted Bunsen flame supplied by a mixture of methane and ethylene. A high-speed camera captures the natural emission of the flame, from which we extract the position of the flame front. We use Bayesian inference to combine this experimental data with our prior knowledge of this flame's behaviour. This prior knowledge is expressed through (i) a model of the kinematics of a flame front moving through a model of the perturbed velocity field, and (ii) a priori estimates of the parameters of the above model with quantified uncertainties. We find the most probable a posteriori model parameters using Bayesian parameter inference, and quantify their uncertainties using Laplace's method combined with first-order adjoint methods. This is substantially cheaper than other common Bayesian inference frameworks, such as Markov Chain Monte Carlo. This process results in a quantitatively-accurate physics-based reduced-order model of the acoustically forced Bunsen flame for injection velocities ranging from 1.75 m/s to 2.99 m/s and equivalence ratio values ranging from 1.26 to 1.47, using seven parameters. We use this model to evaluate the heat release rate between experimental snapshots, to extrapolate to different experimental conditions, and to calculate the flame transfer function and its uncertainty for all the flames. Since the proposed model relies on only seven parameters, it can be trained with little data and successfully extrapolates beyond the training dataset. Matlab code is provided so that the reader can apply it to assimilate further flame images into the model.|
|**2024-07-04**|**ConPR: Ongoing Construction Site Dataset for Place Recognition**|Dongjae Lee et.al.|[2407.03684v1](http://arxiv.org/abs/2407.03684v1)|null|Place recognition, an essential challenge in computer vision and robotics, involves identifying previously visited locations. Despite algorithmic progress, challenges related to appearance change persist, with existing datasets often focusing on seasonal and weather variations but overlooking terrain changes. Understanding terrain alterations becomes critical for effective place recognition, given the aging infrastructure and ongoing city repairs. For real-world applicability, the comprehensive evaluation of algorithms must consider spatial dynamics. To address existing limitations, we present a novel multi-session place recognition dataset acquired from an active construction site. Our dataset captures ongoing construction progress through multiple data collections, facilitating evaluation in dynamic environments. It includes camera images, LiDAR point cloud data, and IMU data, enabling visual and LiDAR-based place recognition techniques, and supporting sensor fusion. Additionally, we provide ground truth information for range-based place recognition evaluation. Our dataset aims to advance place recognition algorithms in challenging and dynamic settings. Our dataset is available at https://github.com/dongjae0107/ConPR.|
|**2024-07-04**|**Sensitivity and Performance of LBTI/NOMIC Spectroscopy: Prospects for 10- and 30-meter class Mid-IR Exoplanet Science**|Brittany E. Miles et.al.|[2407.03620v1](http://arxiv.org/abs/2407.03620v1)|null|Long wavelength infrared (8-13 $\mu$m) spectroscopy is invaluable for detecting molecular features in the atmospheres of gas giant and terrestrial exoplanets. The nulling-optimized mid-infrared camera (NOMIC) on the Large Binocular Telescope Interferometer (LBTI) has a low resolution (R$\sim$200) germanium grism that was previously installed but has not been characterized and commissioned for scientific observations. Using a 1.27 mm slit and broadband filter in combination with the grism, the infrared window between 8-13 $\mu$m can be captured. We describe initial on sky testing of the LBTI/NOMIC grism mode with adaptive optics to study standard stars and binaries. We discuss the impact of observational strategy and telluric calibration on the spectral reduction process. We infer the impact of existing mid-infrared detectors on NOMIC's spectroscopic mode and discuss requirements to enable higher resolution 8-13 $\mu$m spectroscopy on current and future facilities.|
|**2024-07-03**|**Corvus A: A low-mass, isolated galaxy at 3.5 Mpc**|Michael G. Jones et.al.|[2407.03393v1](http://arxiv.org/abs/2407.03393v1)|null|We report the discovery of Corvus A, a low-mass, gas-rich galaxy at a distance of approximately 3.5 Mpc, identified in DR10 of the Dark Energy Camera Legacy Imaging Survey during the initial phase of our ongoing SEmi-Automated Machine LEarning Search for Semi-resolved galaxies (SEAMLESS). Jansky Very Large Array observations of Corvus A detect HI line emission at a radial velocity of $523\pm2$ km/s. Magellan/Megacam imaging reveals an irregular and complex stellar population with both young and old stars. We detect UV emission in Neil Gehrels Swift observations, indicative of recent star formation. However, there are no signs of HII regions in H$\alpha$ imaging from Steward Observatory's Kuiper telescope. Based on the Megacam color magnitude diagram we measure the distance to Corvus A via the tip-of-the-red-giant-branch standard candle as $3.48\pm0.24$ Mpc. This makes Corvus A remarkably isolated, with no known galaxy within $\sim$1 Mpc. Based on this distance, we estimate the HI and stellar mass of Corvus A to be $\log M_\mathrm{HI}/\mathrm{M_\odot} = 6.59$ and $\log M_\ast/\mathrm{M_\odot} = 6.0$. Although there are some signs of rotation, the HI distribution of Corvus A appears to be close to face-on, analogous to that of Leo T, and we therefore do not attempt to infer a dynamical mass from its HI line width. Higher resolution synthesis imaging is required to confirm this morphology and to draw robust conclusions from its gas kinematics.|
|**2024-07-03**|**Advanced Smart City Monitoring: Real-Time Identification of Indian Citizen Attributes**|Shubham Kale et.al.|[2407.03305v2](http://arxiv.org/abs/2407.03305v2)|[link](https://github.com/abhilashk23/vehant-scs-par)|This project focuses on creating a smart surveillance system for Indian cities that can identify and analyze people's attributes in real time. Using advanced technologies like artificial intelligence and machine learning, the system can recognize attributes such as upper body color, what the person is wearing, accessories they are wearing, headgear, etc., and analyze behavior through cameras installed around the city.|
|**2024-07-03**|**Terrain Classification Enhanced with Uncertainty for Space Exploration Robots from Proprioceptive Data**|Mariela De Lucas Álvarez et.al.|[2407.03241v1](http://arxiv.org/abs/2407.03241v1)|null|Terrain Classification is an essential task in space exploration, where unpredictable environments are difficult to observe using only exteroceptive sensors such as vision. Implementing Neural Network classifiers can have high performance but can be deemed untrustworthy as they lack transparency, which makes them unreliable for taking high-stakes decisions during mission planning. We address this by proposing Neural Networks with Uncertainty Quantification in Terrain Classification. We enable our Neural Networks with Monte Carlo Dropout, DropConnect, and Flipout in time series-capable architectures using only proprioceptive data as input. We use Bayesian Optimization with Hyperband for efficient hyperparameter optimization to find optimal models for trustworthy terrain classification.|
|**2024-07-03**|**Streaming Large-Scale Electron Microscopy Data to a Supercomputing Facility**|Samuel S. Welborn et.al.|[2407.03215v1](http://arxiv.org/abs/2407.03215v1)|null|Data management is a critical component of modern experimental workflows. As data generation rates increase, transferring data from acquisition servers to processing servers via conventional file-based methods is becoming increasingly impractical. The 4D Camera at the National Center for Electron Microscopy (NCEM) generates data at a nominal rate of 480 Gbit/s (87,000 frames/s) producing a 700 GB dataset in fifteen seconds. To address the challenges associated with storing and processing such quantities of data, we developed a streaming workflow that utilizes a high-speed network to connect the 4D Camera's data acquisition (DAQ) system to supercomputing nodes at the National Energy Research Scientific Computing Center (NERSC), bypassing intermediate file storage entirely. In this work, we demonstrate the effectiveness of our streaming pipeline in a production setting through an hour-long experiment that generated over 10 TB of raw data, yielding high-quality datasets suitable for advanced analyses. Additionally, we compare the efficacy of this streaming workflow against the conventional file-transfer workflow by conducting a post-mortem analysis on historical data from experiments performed by real users. Our findings show that the streaming workflow significantly improves data turnaround time, enables real-time decision-making, and minimizes the potential for human error by eliminating manual user interactions.|
|**2024-07-03**|**Position and Altitude of the Nao Camera Head from Two Points on the Soccer Field plus the Gravitational Direction**|Stijn Oomes et.al.|[2407.03041v1](http://arxiv.org/abs/2407.03041v1)|[link](https://github.com/physar/landmark_based_slam_dataset)|To be able to play soccer, a robot needs a good estimate of its current position on the field. Ideally, multiple features are visible that have known locations. By applying trigonometry we can estimate the viewpoint from where this observation was actually made. Given that the Nao robots of the Standard Platform League have quite a limited field of view, a given camera frame typically only allows for one or two points to be recognized.   In this paper we propose a method for determining the (x, y) coordinates on the field and the height h of the camera from the geometry of a simplified tetrahedron. This configuration is formed by two observed points on the ground plane plus the gravitational direction. When the distance between the two points is known, and the directions to the points plus the gravitational direction are measured, all dimensions of the tetrahedron can be determined.   By performing these calculations with rational trigonometry instead of classical trigonometry, the computations turn out to be 28.7% faster, with equal numerical accuracy. The position of the head of the Nao can also be externally measured with the OptiTrack system. The difference between externally measured and internally predicted position from sensor data gives us mean absolute errors in the 3-6 centimeters range, when we estimated the gravitational direction from the vanishing point of the outer edges of the goal posts.|
|**2024-07-03**|**Spectroscopic Performance of Detectors for Athena's WFI: Measurements and Simulation**|Johannes Müller-Seidlitz et.al.|[2407.02970v1](http://arxiv.org/abs/2407.02970v1)|null|The depleted p-channel field effect transistor is the chosen sensor type for the Wide Field Imager of the Athena mission. It will be used in two types of cameras. One will enable observations of a field of view of 40' x 40' by using an array of four 512 x 512 pixel sensors in a 2 x 2 configuration. A second, small one is designed to investigate bright, point-like sources with a time resolution of up to 40 microseconds. Sensors of final size, layout, and technology were fabricated, assembled and characterised. Also, first results from the flight production are available and confirm the excellent performance. In order to be able to estimate the future performance of degraded detectors, a simulation was developed that takes into account the non-analytical threshold effects on the basis of measurement results. We present the measurement analysis and the comparison of simulated and measured values as well as first attempts to use the Monte Carlo simulation to predict performance results based on noise measurements.|
|**2024-07-03**|**3D Multimodal Image Registration for Plant Phenotyping**|Eric Stumpe et.al.|[2407.02946v1](http://arxiv.org/abs/2407.02946v1)|[link](https://github.com/eric-stumpe/plant3dimagereg)|The use of multiple camera technologies in a combined multimodal monitoring system for plant phenotyping offers promising benefits. Compared to configurations that only utilize a single camera technology, cross-modal patterns can be recorded that allow a more comprehensive assessment of plant phenotypes. However, the effective utilization of cross-modal patterns is dependent on precise image registration to achieve pixel-accurate alignment, a challenge often complicated by parallax and occlusion effects inherent in plant canopy imaging.   In this study, we propose a novel multimodal 3D image registration method that addresses these challenges by integrating depth information from a time-of-flight camera into the registration process. By leveraging depth data, our method mitigates parallax effects and thus facilitates more accurate pixel alignment across camera modalities. Additionally, we introduce an automated mechanism to identify and differentiate different types of occlusions, thereby minimizing the introduction of registration errors.   To evaluate the efficacy of our approach, we conduct experiments on a diverse image dataset comprising six distinct plant species with varying leaf geometries. Our results demonstrate the robustness of the proposed registration algorithm, showcasing its ability to achieve accurate alignment across different plant types and camera compositions. Compared to previous methods it is not reliant on detecting plant specific image features and can thereby be utilized for a wide variety of applications in plant sciences. The registration approach principally scales to arbitrary numbers of cameras with different resolutions and wavelengths. Overall, our study contributes to advancing the field of plant phenotyping by offering a robust and reliable solution for multimodal image registration.|
|**2024-07-03**|**VEGS: View Extrapolation of Urban Scenes in 3D Gaussian Splatting using Learned Priors**|Sungwon Hwang et.al.|[2407.02945v2](http://arxiv.org/abs/2407.02945v2)|null|Neural rendering-based urban scene reconstruction methods commonly rely on images collected from driving vehicles with cameras facing and moving forward. Although these methods can successfully synthesize from views similar to training camera trajectory, directing the novel view outside the training camera distribution does not guarantee on-par performance. In this paper, we tackle the Extrapolated View Synthesis (EVS) problem by evaluating the reconstructions on views such as looking left, right or downwards with respect to training camera distributions. To improve rendering quality for EVS, we initialize our model by constructing dense LiDAR map, and propose to leverage prior scene knowledge such as surface normal estimator and large-scale diffusion model. Qualitative and quantitative comparisons demonstrate the effectiveness of our methods on EVS. To the best of our knowledge, we are the first to address the EVS problem in urban scene reconstruction. Link to our project page: https://vegs3d.github.io/.|
|**2024-07-03**|**Free-SurGS: SfM-Free 3D Gaussian Splatting for Surgical Scene Reconstruction**|Jiaxin Guo et.al.|[2407.02918v1](http://arxiv.org/abs/2407.02918v1)|[link](https://github.com/wrld/free-surgs)|Real-time 3D reconstruction of surgical scenes plays a vital role in computer-assisted surgery, holding a promise to enhance surgeons' visibility. Recent advancements in 3D Gaussian Splatting (3DGS) have shown great potential for real-time novel view synthesis of general scenes, which relies on accurate poses and point clouds generated by Structure-from-Motion (SfM) for initialization. However, 3DGS with SfM fails to recover accurate camera poses and geometry in surgical scenes due to the challenges of minimal textures and photometric inconsistencies. To tackle this problem, in this paper, we propose the first SfM-free 3DGS-based method for surgical scene reconstruction by jointly optimizing the camera poses and scene representation. Based on the video continuity, the key of our method is to exploit the immediate optical flow priors to guide the projection flow derived from 3D Gaussians. Unlike most previous methods relying on photometric loss only, we formulate the pose estimation problem as minimizing the flow loss between the projection flow and optical flow. A consistency check is further introduced to filter the flow outliers by detecting the rigid and reliable points that satisfy the epipolar geometry. During 3D Gaussian optimization, we randomly sample frames to optimize the scene representations to grow the 3D Gaussian progressively. Experiments on the SCARED dataset demonstrate our superior performance over existing methods in novel view synthesis and pose estimation with high efficiency. Code is available at https://github.com/wrld/Free-SurGS.|
|**2024-07-03**|**Plant Doctor: A hybrid machine learning and image segmentation software to quantify plant damage in video footage**|Marc Josep Montagut Marques et.al.|[2407.02853v1](http://arxiv.org/abs/2407.02853v1)|null|Artificial intelligence has significantly advanced the automation of diagnostic processes, benefiting various fields including agriculture. This study introduces an AI-based system for the automatic diagnosis of urban street plants using video footage obtained with accessible camera devices. The system aims to monitor plant health on a day-to-day basis, aiding in the control of disease spreading in urban areas. By combining two machine vision algorithms, YOLOv8 and DeepSORT, the system efficiently identifies and tracks individual leaves, extracting the optimal images for health analysis. YOLOv8, chosen for its speed and computational efficiency, locates leaves, while DeepSORT ensures robust tracking in complex environments. For detailed health assessment, DeepLabV3Plus, a convolutional neural network, is employed to segment and quantify leaf damage caused by bacteria, pests, and fungi. The hybrid system, named Plant Doctor, has been trained and validated using a diverse dataset including footage from Tokyo urban plants. The results demonstrate the robustness and accuracy of the system in diagnosing leaf damage, with potential applications in large scale urban flora illness monitoring. This approach provides a non-invasive, efficient, and scalable solution for urban tree health management, supporting sustainable urban ecosystems.|
|**2024-07-02**|**Generalized Event Cameras**|Varun Sundar et.al.|[2407.02683v1](http://arxiv.org/abs/2407.02683v1)|null|Event cameras capture the world at high time resolution and with minimal bandwidth requirements. However, event streams, which only encode changes in brightness, do not contain sufficient scene information to support a wide variety of downstream tasks. In this work, we design generalized event cameras that inherently preserve scene intensity in a bandwidth-efficient manner. We generalize event cameras in terms of when an event is generated and what information is transmitted. To implement our designs, we turn to single-photon sensors that provide digital access to individual photon detections; this modality gives us the flexibility to realize a rich space of generalized event cameras. Our single-photon event cameras are capable of high-speed, high-fidelity imaging at low readout rates. Consequently, these event cameras can support plug-and-play downstream inference, without capturing new event datasets or designing specialized event-vision models. As a practical implication, our designs, which involve lightweight and near-sensor-compatible computations, provide a way to use single-photon sensors without exorbitant bandwidth costs.|
|**2024-07-02**|**Investigating Event-Based Cameras for Video Frame Interpolation in Sports**|Antoine Deckyvere et.al.|[2407.02370v2](http://arxiv.org/abs/2407.02370v2)|null|Slow-motion replays provide a thrilling perspective on pivotal moments within sports games, offering a fresh and captivating visual experience. However, capturing slow-motion footage typically demands high-tech, expensive cameras and infrastructures. Deep learning Video Frame Interpolation (VFI) techniques have emerged as a promising avenue, capable of generating high-speed footage from regular camera feeds. Moreover, the utilization of event-based cameras has recently gathered attention as they provide valuable motion information between frames, further enhancing the VFI performances. In this work, we present a first investigation of event-based VFI models for generating sports slow-motion videos. Particularly, we design and implement a bi-camera recording setup, including an RGB and an event-based camera to capture sports videos, to temporally align and spatially register both cameras. Our experimental validation demonstrates that TimeLens, an off-the-shelf event-based VFI model, can effectively generate slow-motion footage for sports videos. This first investigation underscores the practical utility of event-based cameras in producing sports slow-motion content and lays the groundwork for future research endeavors in this domain.|
